# Modeling in R with Tidymodels

```{r, include=FALSE, cache = FALSE}
# knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
library(tidymodels)
```


"The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles."

[https://www.tidymodels.org/](https://www.tidymodels.org/)

Many modeling techniques in R require different syntaxes and different data structures. Tidymodels provides a modeling workflow that standardizes syntaxes and data structures regardless of the model type. 

```{r, eval=F}
lm()
glm()
glmnet()
randomForest()
xgboost()
```

```{r}

c("linear_reg", "logistic_reg", "surv_reg", "multinom_reg", "rand_forest", "boost_tree",
  "svm_poly", "decision_tree") %>% 
  map_dfr(.f = ~show_engines(x = .x) %>% mutate(type = .x)) %>% 
  DT::datatable()

```


## Tidymodels Packages

Like the tidyverse, tidymodels is a 'meta package' consisting of the following packages:

- {[rsample](https://rsample.tidymodels.org/)}: Creates different types of resamples and corresponding classes for analysis
- {[recipes](https://recipes.tidymodels.org/)}: Uses dplyr-like pipeable sequences of feature engineering steps to get data ready for modeling
- {[workflows](https://workflows.tidymodels.org/index.html)}: Creates an object that can bundle together your pre-processing, modeling, and post-processing steps
- {[parsnip](https://parsnip.tidymodels.org/)}: Provides a tidy, unified interface to models than can by used to try a range of models without getting bogged down in the syntactical minutiae of the underlying packages
- {[tune](https://tune.tidymodels.org/)}: Facilitates hyperparameter tuning for the tidymodels packages
- {[yardstick](https://yardstick.tidymodels.org/index.html)}: Estimates how well models are working using tidy data principles
- {[infer](https://infer.tidymodels.org/index.html)}: Performs statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework

### Tidymodels Road Map

What we plan to do:

1. Explore data
   - {dplyr} Manipulate data
   - {ggplot2} Visualize data
2. Create model
   - {rsample} Split data into test/train
   - {recipes} Preprocess data
   - {parsnip} Specify model
   - {workflows} Create workflow
   - {tune} / {dials} Train and tune parameters
   - {parsnip} Finalize model
   - {yardstick} Validate model
3. Predict on new data

### Modeling Goal

We would like to create a model to predict which future Major League Baseball players will make the Hall of Fame.  We will use `historical` data to build a model and then use that model to predict who may make the Hall of Fame from the players in the `eligible` data.

## Explore Data

```{r}
library(tidyverse)

historical <- read_csv("01_data/historical_baseball.csv") %>%
  mutate(inducted = fct_rev(as.factor(inducted))) %>% 
  filter(ab > 250)

eligible <- read_csv("01_data/eligible_baseball.csv") 

historical
```

The `historical` data contains career statistics for every baseball batter from 1880-2011 who no longer meets Hall of Fame eligibility requirements or has already made the Hall of Fame.

Hall of Fame Qualifications:

   - Played at least 10 years
   - Retired for at least 5 years
   - Players have only 10 years of eligibility   

The `eligible` data contains everyone who is still eligible for the Hall of Fame.

You can see from the data below, the players who make the Hall of Fame tend to perform better in a few standard baseball statistics. 

```{r}
historical %>%
  select(-last_year) %>% 
  group_by(inducted) %>% 
  summarise(across(.cols = where(is.numeric), .fns = ~round(mean(.),0))) %>% 
  gt::gt() ## renders the table
```

The plot of the data supports this as well. 

```{r}
historical %>% 
  pivot_longer(g:so) %>% 
  ggplot(aes(x = inducted, y = value)) +
  geom_boxplot() +
  facet_wrap(~name, scales = "free")  +
  labs(y = "",x = "Hall of Fame Indicator")
```

Of note, we are dealing with imbalance classes, which will take unique considerations. To have a quality model, we hope to achieve greater than ~93% accuracy since this is what we could do by simply saying that no one should be in the Hall of Fame. 

```{r}
historical %>% 
  count(inducted) %>% 
  mutate(Percent = str_c(round(n / sum(n),4)*100,"%")) %>% 
  gt::gt()  ## renders the table
```


## Split Data

To begin the analysis, we will load the {tidymodels} library.

```{r}
library(tidymodels)
```

We will split the data into a training (two-thirds of the data) and testing set (one-third) of the data.

We set the seed so the analysis is reproducible.

The output of this function is an rsplit object. An rsplit object is one that can be used with the training and testing functions to extract the data in each split.

```{r}
set.seed(42)

data_split <- initial_split(historical, prop = 2/3, strata = inducted)

data_split
```

We can extract the data from the rsplit object.

```{r}
train_data <- training(data_split)
test_data <- testing(data_split)

train_data
```

From the training data, we further split the data into a training set (two-thirds of the training data) and a validation set (one-third of the training data) for parameter tuning and model assessment.

```{r}
set.seed(42)

validation_set <- validation_split(data = train_data, prop = 2/3, strata = inducted)
validation_set
```

## Prepare Data

What preprocessing steps do you want to do to your data every time you model? 

We need to specify the following things:

   - Specify the modeling formula
   - Specify the 'roles' of each of the factors
   - Do all preprocessing steps 

In the {tidymodels} construct, we do this by creating a recipe.

```{r}

baseball_recipe <-
  recipe(inducted ~ ., data = train_data) %>% 
  update_role(player_id, new_role = "ID") %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>% 
  step_nzv(all_numeric()) %>% 
  step_rm("last_year")

baseball_recipe
```

## Specify Model

Now that we've prepared our data, we need to specify the model we wish to execute.

Here we identify the model type, specify parameters that need tuning, and then set our desired 'engine' (essentially, the modeling algorithm).

```{r}
lr_mod <-
  logistic_reg(mode = "classification", penalty = tune(), mixture = 1) %>% 
  set_engine(engine = "glmnet")

lr_mod
```

## Create Workflow

Now that we've prepared the data and specified the model, we put it all together in a workflow.

In a workflow, we add the specified model and the preprocessing recipe.

```{r}
baseball_workflow <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(baseball_recipe)

baseball_workflow
```

## Specify Grid of Training Parameters

This step not only executes the model building procedure, but also tunes the penalty hyperparameter by running the model with every penalty option in a specified search grid.

First, we specify the parameters and search grids that we'll use for tuning.

```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_reg_grid
```

## Train Model

Next, we use `tune_grid()` to execute the model one time for each parameter set. In this instance, this is 30 times.

This function has several arguments:

  - `grid`: The tibble we created that contains the parameters we have specified.
  - `control`: Controls various aspects of the grid search process. 
  - `metrics`: Specifies the model quality metrics we wish to save for each model in cross validation.

We also specify that we wish to save the performance metrics for each of the 30 iterations. 

```{r}
set.seed(42)

lr_validation <-
  baseball_workflow %>% 
  tune_grid(validation_set,
            grid = lr_reg_grid, 
            control = control_grid(save_pred = TRUE, 
                                   verbose = TRUE, 
                                   allow_par = FALSE),
            metrics = metric_set(roc_auc, accuracy))

lr_validation
```


Here, we extract out the best 25 models based on accuracy and plot them versus the penalty from the tuning parameter grid.

```{r}
lr_validation %>% 
  show_best("accuracy", n = 25) %>% 
  arrange(penalty) %>% as.data.frame() %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10()
```

We select the smallest penalty that results in the highest accuracy.

```{r}
lr_best <-
  lr_validation %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  filter(mean == max(mean)) %>% 
  slice(1)
```

We show the Receiver Operator Characteristic (ROC) curve for the selected model.

```{r}
lr_validation %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()

```


## Build Model on all Training Data, Test on Validation Set

Now that we've found the best parameter set, we need to apply this model to the entire training set.

We'll make one tweak to our previous model specification: we specify our chosen penalty from the tuning process, instead of allowing the penalty to be tuned automatically. 

```{r}
last_lr_mod <-
  logistic_reg(mode = "classification", penalty = lr_best$penalty, mixture = 1) %>% 
  set_engine(engine = "glmnet")

last_lr_mod
```

We update our workflow to have the best parameter set with the function `finalize_workflow()`.

```{r}
last_lr_workflow <-
  baseball_workflow %>%
  finalize_workflow(lr_best)

last_lr_workflow
```

We fit the model on the entire training set.

```{r}
last_lr_fit <-
  last_lr_workflow %>% 
  last_fit(data_split)
```

To look at the model output, we can 'tidy()' the 'fit()'

```{r}
last_lr_workflow %>% 
  fit(data = historical) %>% 
  tidy()
```

We can see the performance of the model below in the next two outputs.

```{r}
last_lr_fit %>% 
  collect_metrics()

last_lr_workflow %>% 
  fit(data = historical) %>%
  predict(historical, type = "prob") %>% 
  bind_cols(historical) %>% 
  mutate(pred_class = fct_rev(as.factor(round(.pred_1)))) %>% 
  conf_mat(inducted, pred_class)
```

And we can take a view look at the ROC curve of our final model.

```{r}
last_lr_fit %>% 
  collect_predictions() %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()
```

## Change Model to Random Forest

### Update Model Type

```{r}
rf_mod <-
  rand_forest(mode = "classification", mtry = tune(), min_n = tune(), trees = tune()) %>% 
  set_engine(engine = "randomForest")

rf_mod
```

### Update Workflow

```{r}
baseball_workflow_rf <-
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(baseball_recipe)

baseball_workflow_rf
```

### Update Tuning Parameters

```{r}
rf_reg_grid <- dials::grid_latin_hypercube(mtry(c(1,10)), min_n(), trees(), size = 30)

rf_reg_grid
```

### Re Execute Cross Validation

```{r}
set.seed(42)

rf_validation <-
  baseball_workflow_rf %>% 
  tune_grid(validation_set,
            grid = rf_reg_grid, 
            control = control_grid(save_pred = TRUE, 
                                   verbose = TRUE, 
                                   allow_par = FALSE),
            metrics = metric_set(roc_auc, accuracy))

rf_validation
```

### Explore Tuning Parameters

```{r}
rf_validation %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  pivot_longer(cols = mtry:min_n) %>%
  mutate(best_mod = mean == max(mean)) %>% 
  ggplot(aes(x = value, y = mean)) +
  # geom_line(alpha = 0.5, size = 1.5) +
  geom_point(aes(color = best_mod)) +
  facet_wrap(~name, scales = "free_x") +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  labs(y = "Accuracy", x = "", color = "Best Model", title = "Random Forest Cross Validation Tuning Parameters")
```

### Select Best Tuning Parameters for Random Forest

```{r}
rf_best <-
  rf_validation %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  arrange(desc(mean)) %>% 
  slice(1)
```

### Show ROC Curve for Best Random Forest Model

```{r}
rf_validation %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()
```

```{r}
last_rf_mod <-
  rand_forest(mode = "classification", mtry = rf_best$mtry, trees = rf_best$trees, min_n = rf_best$min_n) %>% 
  set_engine(engine = "randomForest")

last_rf_mod
```

We update our workflow to have the best parameter set with the function `finalize_workflow()`.

```{r}
last_rf_workflow <-
  baseball_workflow_rf %>%
  finalize_workflow(rf_best)

last_rf_workflow
```

We fit the model on the entire training set.

```{r}
last_rf_fit <-
  last_rf_workflow %>% 
  last_fit(data_split)
```

We can see the performance of the model below in the next two outputs.

```{r}
last_rf_fit %>% 
  collect_metrics()

last_rf_workflow %>% 
  fit(data = historical) %>%
  predict(historical, type = "prob") %>% 
  bind_cols(historical) %>% 
  mutate(pred_class = fct_rev(as.factor(round(.pred_1)))) %>% 
  conf_mat(inducted, pred_class)
```

And we can take a view look at the ROC curve of our final model.

```{r}
last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()
```


## Build Model on all Training and Validation Data Using the Best Model

Now, we can use the `fit()` function to build the model on the entire `historical` data. 

```{r}
last_rf_workflow %>% 
  fit(data = historical) %>%
  extract_fit_parsnip() 

```

## Make Predictions with New Data

Now that we have the model, we can make predictions on the `eligible` data.

How did we do?


```{r}
last_rf_workflow %>% 
  fit(data = historical) %>%
  predict(eligible, type = "prob") %>% 
  bind_cols(eligible) %>% 
  arrange(-.pred_1) %>% 
  filter(.pred_1 >.4) %>%
  mutate(across(contains("pred"), ~round(.,3))) %>% 
  # print(n = Inf) %>% 
  DT::datatable()
```

