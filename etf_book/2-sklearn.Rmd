# Modeling in python with scikit-learn

"The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles."

[https://www.tidymodels.org/](https://www.tidymodels.org/)

Many modeling techniques in R require different syntax and different data structures.  Tidymodels provides modeling workflow that standardizes syntax and data structures regardless of the model type.  

## Tidymodels Packages

Like the tidyverse, tidymodels is a 'meta package' consisting of the following packages:

- {[rsample](https://rsample.tidymodels.org/)}:  Creates different types of resamples and corresponding classes for analysis
- {[recipes](https://recipes.tidymodels.org/)}:  Uses dplyr-like pipeable sequences of feature engineering steps to get data ready for modeling
- {[workflows](https://workflows.tidymodels.org/index.html)}:   Creates an object that can bundle together your pre-processing, modeling, and post-processing steps
- {[parsnip](https://parsnip.tidymodels.org/)}: Provides a tidy, unified interface to models than can by used to try a range of models without getting bogged down in the syntactical ninutae of the underslying packages.
- {[tune](https://tune.tidymodels.org/)}: Facilitates hyperparameter tuning for the tidymodels packages.
- {[yardstick](https://yardstick.tidymodels.org/index.html)}: Estimates how well models are working using tidy data principles.
- {[infer](https://infer.tidymodels.org/index.html)}: Performs statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework.

### Tidymodels Road Map

What we plan to do:

1. Explore data
2. Create model
   - {rsample} Split data into test/train
   - {recipies} Preprocess data
   - {parsnip} Specify model
   - {workflows} Create workflow
   - {tune} / {dials} Train and tune parameters
   - {parsnip} Finalize model
   - {yardstick} Validate model
3. Predict on new data

### Modeling Goal

We desire to create a model using the `historical` data and use that model to predict who may make the Hall-of-Fame in the `eligible` data.

## Explore Data

```{r}
library(reticulate)
library(tidyverse)
```
```{python}
import pandas as pd
```
```{python}
historical = pd.read_csv('01_data/historical_baseball.csv')
eligible = pd.read_csv('01_data/eligible_baseball.csv')

historical
```
The `historical` data contains career statistics of every baseball batter from 1880-2011 who no longer meet Hall-of-Fame eligibility requirements or who have already made the hall-of-fame.

Hall of Fame Qualifications  
- Play at least 10 years  
- Retired for at least 5 years  
- Players have only 10 years of eligability   

The `eligible` data contains everyone who is still eligible for the Hall-of-Fame

You can see from the data below, the players who make the Hall-of-Fame tend to perform better in a few standard baseball statistics. 

```{python}
hist_means_inducted_groups = historical.drop('last_year', axis = 1) \
  .groupby('inducted') \
  .mean() \
  .round()
```
```{r}
py$hist_means_inducted_groups %>%
  rownames_to_column(var = "inducted") %>%
  gt::gt()
```

The plot of the data supports this as well. 

```{r}
historical %>% 
  pivot_longer(g:so) %>% 
  ggplot(aes(x = inducted, y = value)) +
  geom_boxplot() +
  facet_wrap(~name, scales = "free")  +
  labs(y = "",x = "Hall-of-Fame Indicator")
```

## Split Data

We will split the data into a training (2/3s of the data) and testing set (1/3) of the data.

We set the seed so the analysis is reproducible.  

The output of this function is an rsplit object.  An rsplit object that can be used with the training and testing functions to extract the data in each split.

```{r}
set.seed(42)

data_split <- initial_split(historical, prop = 2/3, strata = inducted)

data_split
```
```{python}
from sklearn.model_selection import train_test_split

historical_pidindex = historical.set_index('player_id')

X = historical_pidindex.drop(('inducted', 'last_year', axis = 1))
y = historical_pidindex.inducted

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size = 1/3)
```
We can extract the data from the rsplit object.


NOTE: Not sure I want to do this. Need to look into how gridsearchcv works more carefully first.

```{python}
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

l1_ratio

pipe_scale_lr_lasso = make_pipeline(StandardScaler(), LogisticRegressionCV(penalty = "elasticnet", solver = "saga", l1_ratios = [1.0], cv = 10, max_iter = 1000))
pipe_scale_lr_lasso.fit(X_train, y_train)  # apply scaling on training data
pipe_scale_lr_lasso.score(X_test, y_test)

pipe_scale_lr = make_pipeline(StandardScaler(), LogisticRegression())
pipe_scale_lr.fit(X_train, y_train)  # apply scaling on training data

pipe_scale_lr.score(X_test, y_test)
```



## Prepare Data

What preprocessing steps do you want to do to your date every time you model?  

We need to specify the following things:
- Specify the modeling formula  
- Specify the 'roles' of each of the factors  
- Do all preprocessing steps  

In the {tidymodels} construct, we do this by creating a recipe.

```{r}

baseball_recipie <-
  recipe(inducted ~ ., data = train_data) %>% 
  update_role(player_id, new_role = "ID") %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>% 
  step_nzv(all_numeric()) %>% 
  step_rm("last_year")

baseball_recipie
```

## Specify Model

Now that we've prepared our data, we need to specify the model we wish to execute.

Here we identify the model type, specify parameters which need tuning, and then set the 'engine' (package) we wish to do the work.

```{r}
lr_mod <-
  logistic_reg(mode = "classification", penalty = tune(), mixture = 1) %>% 
  set_engine(engine = "glmnet")

lr_mod
```

## Create Workflow

Now that we've prepared the data and specified the model, we put it all together in a workflow.

In a workflow, we add the specified model and the preprocessing recipe.

```{r}
baseball_workflow <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(baseball_recipie)

baseball_workflow
```

## Train Model and Tune Parameters

This step not only executes the model building proceedure, but it contains the structure to execute the model over a grid of parameters to tune the model and find the best parameter set.

First we specify the parameters over which we desire to tune.

```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
```

Next, we use `tune_grid()` to execute the model one time for each parameter set.  In this instance, this is 30 times.

This function has several arguments

`grid`: The tibble we created that contains the parameters we have specified.
`control`: Controls the aspects of the grid search process.  
`metrics`: Specifies the model quality metrics we wish to save for each model in cross validation.

We also specify that we wish to save the performance metrics for each of the 30 iterations.  

```{r}
set.seed(42)

lr_validation <-
  baseball_workflow %>% 
  tune_grid(validation_set,
            grid = lr_reg_grid, 
            control = control_grid(save_pred = TRUE, 
                                   verbose = TRUE, 
                                   allow_par = FALSE),
            metrics = metric_set(roc_auc, accuracy))

lr_validation
```


Here, we extract out the best 25 models based on accuracy and plot them vs the pentalty from the tuning parameter grid.

```{r}
lr_validation %>% 
  show_best("accuracy", n = 25) %>% 
  arrange(penalty) %>% as.data.frame() %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10()
```

We desire the pentalty that gives us the simplest model, with the best accuracy.  Because of that, we select the 7th smallest penalty.

```{r}
lr_best <-
  lr_validation %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  slice(7)
```

We show the ROC curve for the selected model.

```{r}
lr_validation %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()

```


## Build model on all training data, test on validation set.

Now that we've found the best parameter set, we need to apply this model to the entire training dataset.

We made one tweek to our model specification.  We specify the specific penalry from our best model from cross validation. 

```{r}
last_lr_mod <-
  logistic_reg(mode = "classification", penalty = lr_best$penalty, mixture = 1) %>% 
  set_engine(engine = "glmnet")

last_lr_mod
```

We update our workflow to have the best parameter set with the function `finalize_workflow()`.

```{r}
last_lr_workflow <-
  baseball_workflow %>%
  finalize_workflow(lr_best)

last_lr_workflow
```

We fit the model on the entire training set.

```{r}
last_lr_fit <-
  last_lr_workflow %>% 
  last_fit(data_split)
```

We can see the performance of the model below.

```{r}
last_lr_fit %>% 
  collect_metrics()
```

And we can take a view look at the ROC of our final model.

```{r}
last_lr_fit %>% 
  collect_predictions() %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()
```

## Build model on all training and validation data

Now, we can use the `fit()` function to build the model on the enitre `historical` data. 

```{r}
last_lr_workflow %>% 
  fit(data = historical) %>%
  pull_workflow_fit() %>% tidy()
```

Now that we have the model, we can make predictions on the `eligible` data.

How did we do?


```{r}
last_lr_workflow %>% 
  fit(data = historical) %>%
  predict(eligible, type = "prob") %>% 
  bind_cols(eligible) %>% 
  arrange(-.pred_1) %>% 
  filter(.pred_1 >.4) %>%
  mutate(across(contains("pred"), ~round(.,3))) %>% 
  # print(n = Inf) %>% 
  DT::datatable()
```

