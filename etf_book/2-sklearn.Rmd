# Modeling in python with scikit-learn

## Scikit-learn Overview

Like {tidymodels}, scikit-learn offers a suite of tools for predictive modeling and machine learning: it will help you split data, preprocess model inputs, fit models, and compare and assess them.

Unlike {tidymodels}, scikit-learn is a single monolithic package with functions for the entire modeling pipeline. Users will likely still want to use pandas to ingest and prepare data, and may want to use other libraries to supplement scikit-learn's data visualiation capabilities, but scikit-learn will do most of the work by itself - and often with less and simpler code than tidymodels, at least for standard machine learning workflows.

### Tidymodels Road Map

What we plan to do:

1. Read in and explore data (pandas and R)
2. Create model (scikit-learn)
   - split data
   - define pipeline with preprocessors and model with cross-validation for parameter tuning
   - fit model
3. Predict on new data and assess model (scikit-learn)

### Modeling Goal

We plan to create a model using the `historical` data and use that model to predict who is most likely to make it into the Hall of Fame in the `eligible` data.

## Explore Data

```{r include=FALSE}
library(reticulate)
library(tidyverse)
```
We'll load the pandas library to import and set up the data.

```{python}
import pandas as pd
```
Here, we use panda's `read_csv()` to import the data, and then we print the first few rows of the historical dataframe to the console.

```{python}
historical = pd.read_csv('01_data/historical_baseball.csv')
eligible = pd.read_csv('01_data/eligible_baseball.csv')

historical
```
The `historical` data contains career statistics of every baseball batter from 1880-2011 who no longer meet Hall-of-Fame eligibility requirements or who have already made the hall-of-fame.

Hall of Fame Qualifications  
- Play at least 10 years  
- Retired for at least 5 years  
- Players have only 10 years of eligability   

The `eligible` data contains everyone who is still eligible for the Hall-of-Fame

You can see from the data below, the players who make the Hall-of-Fame tend to perform better in a few standard baseball statistics. This pandas code does the same thing as the R code in the previous chapter - it groups the historical data by whether or not the player was inducted into the Hall of Fame, and then takes the mean of each column in each group.

```{python}
hist_means_inducted_groups = historical.drop('last_year', axis = 1) \
  .groupby('inducted') \
  .mean() \
  .round()
```
We bring the data back into R, using RStudio's very simple Python-R interface, and use the same R code as in Chapter 1 to print a table and produce boxplots of the means.

```{r}
py$hist_means_inducted_groups %>%
  rownames_to_column(var = "inducted") %>%
  gt::gt()
```

```{r}
historical %>% 
  pivot_longer(g:so) %>% 
  ggplot(aes(x = inducted, y = value)) +
  geom_boxplot() +
  facet_wrap(~name, scales = "free")  +
  labs(y = "",x = "Hall-of-Fame Indicator")
```

## Split Data

As we did in R, we will split the data into a training set (2/3s of the data) and testing set (1/3) of the data.

We set the seed so the analysis is reproducible - here, we do this using the `random_state` parameter in `train_test_split()`.

Instead of an `rsplit` object that contains resampling metadata, `train_test_split()` returns four objects: X (predictor) pandas DataFrame objects for the training and test sets, and y (target) pandas Series objects for the training and test sets.

Note that before splitting the data, we set the index of the dataframe to be `player_id`. This carries through to the outputs of `train_test_split()`, which all have `player_id` as a common index (and not as a predictor or target variable.) In a way, this serves a similar purpose to the `update_role(player_id, new_role = "ID")` line that we added to the recipe in R.

```{python}
from sklearn.model_selection import train_test_split

historical_pidindex = historical.set_index('player_id')

X = historical_pidindex.drop(['inducted', 'last_year'], axis = 1)
y = historical_pidindex.inducted

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size = 1/3)
```

## Define a Pipeline

Scikit-learn's "pipelines" serve the combined purpose of "workflows" and "recipes" in {tidymodels}. They allow you to define a set of preprocessing and modeling steps that you can then apply to any dataset. They are defined by the function `make_pipeline()`, with the steps, in order, as arguments.

The first step in our pipeline will take care of scaling and centering our data. In the previous chapter, we  be using `StandardScaler()`,

NOTE: Not sure I want to do this. Need to look into how gridsearchcv works more carefully first.

```{python}
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# l1_ratio

pipe_scale_lr_lasso = make_pipeline(StandardScaler(), LogisticRegressionCV(penalty = "elasticnet", solver = "saga", l1_ratios = [1.0], cv = 10, max_iter = 1000))
pipe_scale_lr_lasso.fit(X_train, y_train)  # apply scaling on training data
pipe_scale_lr_lasso.score(X_test, y_test)

pipe_scale_lr = make_pipeline(StandardScaler(), LogisticRegression())
pipe_scale_lr.fit(X_train, y_train)  # apply scaling on training data

pipe_scale_lr.score(X_test, y_test)
```



## Prepare Data

What preprocessing steps do you want to do to your date every time you model?  

We need to specify the following things:
- Specify the modeling formula  
- Specify the 'roles' of each of the factors  
- Do all preprocessing steps  

In the {tidymodels} construct, we do this by creating a recipe.

```{r}

baseball_recipie <-
  recipe(inducted ~ ., data = train_data) %>% 
  update_role(player_id, new_role = "ID") %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>% 
  step_nzv(all_numeric()) %>% 
  step_rm("last_year")

baseball_recipie
```

## Specify Model

Now that we've prepared our data, we need to specify the model we wish to execute.

Here we identify the model type, specify parameters which need tuning, and then set the 'engine' (package) we wish to do the work.

```{r}
lr_mod <-
  logistic_reg(mode = "classification", penalty = tune(), mixture = 1) %>% 
  set_engine(engine = "glmnet")

lr_mod
```

## Create Workflow

Now that we've prepared the data and specified the model, we put it all together in a workflow.

In a workflow, we add the specified model and the preprocessing recipe.

```{r}
baseball_workflow <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(baseball_recipie)

baseball_workflow
```

## Train Model and Tune Parameters

This step not only executes the model building proceedure, but it contains the structure to execute the model over a grid of parameters to tune the model and find the best parameter set.

First we specify the parameters over which we desire to tune.

```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
```

Next, we use `tune_grid()` to execute the model one time for each parameter set.  In this instance, this is 30 times.

This function has several arguments

`grid`: The tibble we created that contains the parameters we have specified.
`control`: Controls the aspects of the grid search process.  
`metrics`: Specifies the model quality metrics we wish to save for each model in cross validation.

We also specify that we wish to save the performance metrics for each of the 30 iterations.  

```{r}
set.seed(42)

lr_validation <-
  baseball_workflow %>% 
  tune_grid(validation_set,
            grid = lr_reg_grid, 
            control = control_grid(save_pred = TRUE, 
                                   verbose = TRUE, 
                                   allow_par = FALSE),
            metrics = metric_set(roc_auc, accuracy))

lr_validation
```


Here, we extract out the best 25 models based on accuracy and plot them vs the pentalty from the tuning parameter grid.

```{r}
lr_validation %>% 
  show_best("accuracy", n = 25) %>% 
  arrange(penalty) %>% as.data.frame() %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10()
```

We desire the pentalty that gives us the simplest model, with the best accuracy.  Because of that, we select the 7th smallest penalty.

```{r}
lr_best <-
  lr_validation %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  slice(7)
```

We show the ROC curve for the selected model.

```{r}
lr_validation %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()

```


## Build model on all training data, test on validation set.

Now that we've found the best parameter set, we need to apply this model to the entire training dataset.

We made one tweek to our model specification.  We specify the specific penalry from our best model from cross validation. 

```{r}
last_lr_mod <-
  logistic_reg(mode = "classification", penalty = lr_best$penalty, mixture = 1) %>% 
  set_engine(engine = "glmnet")

last_lr_mod
```

We update our workflow to have the best parameter set with the function `finalize_workflow()`.

```{r}
last_lr_workflow <-
  baseball_workflow %>%
  finalize_workflow(lr_best)

last_lr_workflow
```

We fit the model on the entire training set.

```{r}
last_lr_fit <-
  last_lr_workflow %>% 
  last_fit(data_split)
```

We can see the performance of the model below.

```{r}
last_lr_fit %>% 
  collect_metrics()
```

And we can take a view look at the ROC of our final model.

```{r}
last_lr_fit %>% 
  collect_predictions() %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()
```

## Build model on all training and validation data

Now, we can use the `fit()` function to build the model on the enitre `historical` data. 

```{r}
last_lr_workflow %>% 
  fit(data = historical) %>%
  pull_workflow_fit() %>% tidy()
```

Now that we have the model, we can make predictions on the `eligible` data.

How did we do?


```{r}
last_lr_workflow %>% 
  fit(data = historical) %>%
  predict(eligible, type = "prob") %>% 
  bind_cols(eligible) %>% 
  arrange(-.pred_1) %>% 
  filter(.pred_1 >.4) %>%
  mutate(across(contains("pred"), ~round(.,3))) %>% 
  # print(n = Inf) %>% 
  DT::datatable()
```

