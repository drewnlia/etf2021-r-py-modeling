# Modeling in python with scikit-learn

## Scikit-learn Overview

Like {tidymodels}, scikit-learn offers a suite of tools for predictive modeling and machine learning: it will help you split data, preprocess model inputs, fit models, and compare and assess them.

Unlike {tidymodels}, scikit-learn is a single monolithic package with functions for the entire modeling pipeline. Users will likely still want to use pandas to ingest and prepare data, and may want to use other libraries to supplement scikit-learn's data visualiation capabilities, but scikit-learn will do most of the work by itself - and often with less and simpler code than tidymodels, at least for standard machine learning workflows.

### Tidymodels Road Map

What we plan to do:

1. Read in and explore data (pandas and R)
2. Create model (scikit-learn)
   - split data
   - define pipeline with preprocessors and model with cross-validation for parameter tuning
   - fit model
3. Predict on new data and assess model (scikit-learn)

### Modeling Goal

We plan to create a model using the `historical` data and use that model to predict who is most likely to make it into the Hall of Fame in the `eligible` data.

## Explore Data

```{r include=FALSE}
library(reticulate)
library(tidyverse)
```
We'll load the pandas library to import and set up the data.

```{python}
import pandas as pd
import numpy as np
```
Here, we use panda's `read_csv()` to import the data, and then we print the first few rows of the historical dataframe to the console.

```{python}
historical = pd.read_csv('01_data/historical_baseball.csv')
eligible = pd.read_csv('01_data/eligible_baseball.csv')

historical
```
The `historical` data contains career statistics of every baseball batter from 1880-2011 who no longer meet Hall-of-Fame eligibility requirements or who have already made the hall-of-fame.

Hall of Fame Qualifications  
- Play at least 10 years  
- Retired for at least 5 years  
- Players have only 10 years of eligability   

The `eligible` data contains everyone who is still eligible for the Hall-of-Fame

You can see from the data below, the players who make the Hall-of-Fame tend to perform better in a few standard baseball statistics. This pandas code does the same thing as the R code in the previous chapter - it groups the historical data by whether or not the player was inducted into the Hall of Fame, and then takes the mean of each column in each group.

```{python}
hist_means_inducted_groups = historical.drop('last_year', axis = 1) \
  .groupby('inducted') \
  .mean() \
  .round()
```
We bring the data back into R, using RStudio's very simple Python-R interface, and use the same R code as in Chapter 1 to print a table and produce boxplots of the means.

```{r}
py$hist_means_inducted_groups %>%
  rownames_to_column(var = "inducted") %>%
  gt::gt()
```

```{r}
historical %>% 
  pivot_longer(g:so) %>% 
  ggplot(aes(x = inducted, y = value)) +
  geom_boxplot() +
  facet_wrap(~name, scales = "free")  +
  labs(y = "",x = "Hall-of-Fame Indicator")
```

## Split Data

As we did in R, we will split the data into a training set (2/3s of the data) and testing set (1/3) of the data.

We set the seed so the analysis is reproducible - here, we do this using the `random_state` parameter in `train_test_split()`.

Instead of an `rsplit` object that contains resampling metadata, `train_test_split()` returns four objects: X (predictor) pandas DataFrame objects for the training and test sets, and y (target) pandas Series objects for the training and test sets.

Note that before splitting the data, we set the index of the dataframe to be `player_id`. This carries through to the outputs of `train_test_split()`, which all have `player_id` as a common index (and not as a predictor or target variable.) In a way, this serves a similar purpose to the `update_role(player_id, new_role = "ID")` line that we added to the recipe in R.

```{python}
from sklearn.model_selection import train_test_split

historical_pidindex = historical.set_index('player_id')

X = historical_pidindex.drop(['inducted', 'last_year'], axis = 1)
y = historical_pidindex.inducted

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size = 1/3)
```

## Define a Pipeline

Scikit-learn's "pipelines" serve the combined purpose of "workflows" and "recipes" in {tidymodels}. They allow you to define a set of preprocessing and modeling steps that you can then apply to any dataset. They are defined by the function `make_pipeline()`, with the steps, in order, as arguments.

The first two steps in our pipeline will take care of preprocessing. In the previous chapter, we centered and scaled our data; here, we'll use `StandardScaler()`, which accomplishes both of those steps. We'll also apply `VarianceThreshold()`; in its default form, this only removes zero-variance predictors, but the user can set a custom variance threshold. None of our predictors have low variance, so this feature selection mechanism does nothing anyway.

The third step in our pipeline is our model. Here, we've chosen `LogisticRegressionCV()`. The first three parameters should produce a model very similar to the one in the previous chapter:
  - `penalty = "elasticnet"` lets us use a hybrid L1 and L2 penalty, or a mix between Lasso and Ridge regression, much like `engine = glmnet` in R;
  - `solver = "saga"` chooses a solver that is compatible with our other options;
  - `l1_ratios = [1.0]` is the equivalent of `mixture = 1` in R - it gives us a pure Lasso regression;
  - `max_iter = 1000` allows the solver to attempt up to 1000 iterations as it searches for a solution. The default of 100 was insufficient for this data. 

We also have one parameter related to the cross-validation (CV) part of the model specification: `cv = 10`. This means that the data will be split into ten folds, and the model will be fit ten times, with one fold being held out as a validation set each time. This process will allow the model to tune the size of penalty, which we have not specified explicitly.

**NOTE: MAKE SURE THAT I'M CORRECTLY PORTRAYING HOW LOGISTICREGRESSIONCV USES CROSS VALIDATION**

```{python}
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold

# l1_ratio

pipe_scale_lr_lasso = make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegressionCV(penalty = "elasticnet", solver = "saga", l1_ratios = [1.0], cv = 10, max_iter = 1000))
```

It is also possible to use a parameter tuning method more like the onein the previous chapter, using `gridsearchCV` and a predefined grid of search values The scikit-learn user guide has a very detailed section on this method, available at: https://scikit-learn.org/stable/modules/grid_search.html

## Fit the Model (Using the Pipeline)

With our pipeline defined, fitting the model on the training data is very easy: we simply call the `fit()` method on the pipeline, with our `X_train` and `y_train` data as the inputs.

```{python}
pipe_scale_lr_lasso.fit(X_train, y_train)  # apply scaling on training data
```


## Score and Evaluate the Model

The most basic way to assess the performance of a fitted scikit-learn model is the `score()` function, with the test set as inputs. This uses the fitted model to predict on the test set and returns the proportion of correct predictions.

```{python}
pipe_scale_lr_lasso.score(X_test, y_test)
```

```{python}

pipe_scale_lr = make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegression())
pipe_scale_lr.fit(X_train, y_train)  # apply scaling on training data

pipe_scale_lr.score(X_test, y_test)
```



## Prepare Data

What preprocessing steps do you want to do to your date every time you model?  

We need to specify the following things:
- Specify the modeling formula  
- Specify the 'roles' of each of the factors  
- Do all preprocessing steps  

In the {tidymodels} construct, we do this by creating a recipe.

```{r}

baseball_recipie <-
  recipe(inducted ~ ., data = train_data) %>% 
  update_role(player_id, new_role = "ID") %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>% 
  step_nzv(all_numeric()) %>% 
  step_rm("last_year")

baseball_recipie
```

## Specify Model

Now that we've prepared our data, we need to specify the model we wish to execute.

Here we identify the model type, specify parameters which need tuning, and then set the 'engine' (package) we wish to do the work.

```{r}
lr_mod <-
  logistic_reg(mode = "classification", penalty = tune(), mixture = 1) %>% 
  set_engine(engine = "glmnet")

lr_mod
```

## Create Workflow

Now that we've prepared the data and specified the model, we put it all together in a workflow.

In a workflow, we add the specified model and the preprocessing recipe.

```{r}
baseball_workflow <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(baseball_recipie)

baseball_workflow
```

## Train Model and Tune Parameters

This step not only executes the model building proceedure, but it contains the structure to execute the model over a grid of parameters to tune the model and find the best parameter set.

First we specify the parameters over which we desire to tune.

```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
```

Next, we use `tune_grid()` to execute the model one time for each parameter set.  In this instance, this is 30 times.

This function has several arguments

`grid`: The tibble we created that contains the parameters we have specified.
`control`: Controls the aspects of the grid search process.  
`metrics`: Specifies the model quality metrics we wish to save for each model in cross validation.

We also specify that we wish to save the performance metrics for each of the 30 iterations.  

```{r}
set.seed(42)

lr_validation <-
  baseball_workflow %>% 
  tune_grid(validation_set,
            grid = lr_reg_grid, 
            control = control_grid(save_pred = TRUE, 
                                   verbose = TRUE, 
                                   allow_par = FALSE),
            metrics = metric_set(roc_auc, accuracy))

lr_validation
```


Here, we extract out the best 25 models based on accuracy and plot them vs the pentalty from the tuning parameter grid.

```{r}
lr_validation %>% 
  show_best("accuracy", n = 25) %>% 
  arrange(penalty) %>% as.data.frame() %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10()
```

We desire the pentalty that gives us the simplest model, with the best accuracy.  Because of that, we select the 7th smallest penalty.

```{r}
lr_best <-
  lr_validation %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  slice(7)
```

We show the ROC curve for the selected model.

```{r}
lr_validation %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()

```


## Build model on all training data, test on validation set.

Now that we've found the best parameter set, we need to apply this model to the entire training dataset.

We made one tweek to our model specification.  We specify the specific penalry from our best model from cross validation. 

```{r}
last_lr_mod <-
  logistic_reg(mode = "classification", penalty = lr_best$penalty, mixture = 1) %>% 
  set_engine(engine = "glmnet")

last_lr_mod
```

We update our workflow to have the best parameter set with the function `finalize_workflow()`.

```{r}
last_lr_workflow <-
  baseball_workflow %>%
  finalize_workflow(lr_best)

last_lr_workflow
```

We fit the model on the entire training set.

```{r}
last_lr_fit <-
  last_lr_workflow %>% 
  last_fit(data_split)
```

We can see the performance of the model below.

```{r}
last_lr_fit %>% 
  collect_metrics()
```

And we can take a view look at the ROC of our final model.

```{r}
last_lr_fit %>% 
  collect_predictions() %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()
```

## Build model on all training and validation data

Now, we can use the `fit()` function to build the model on the enitre `historical` data. 

```{r}
last_lr_workflow %>% 
  fit(data = historical) %>%
  pull_workflow_fit() %>% tidy()
```

Now that we have the model, we can make predictions on the `eligible` data.

How did we do?


```{r}
last_lr_workflow %>% 
  fit(data = historical) %>%
  predict(eligible, type = "prob") %>% 
  bind_cols(eligible) %>% 
  arrange(-.pred_1) %>% 
  filter(.pred_1 >.4) %>%
  mutate(across(contains("pred"), ~round(.,3))) %>% 
  # print(n = Inf) %>% 
  DT::datatable()
```

