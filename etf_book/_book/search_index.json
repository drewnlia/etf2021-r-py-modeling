[["2-modeling-in-r-with-tidymodels.html", "2 Modeling in R With Tidymodels 2.1 Tidymodels Packages 2.2 Explore Data 2.3 Split Data 2.4 Prepare Data 2.5 Specify Model 2.6 Create Workflow 2.7 Specify Grid of Training Parameters 2.8 Train Model 2.9 Build Model on all Training Data, Test on Validation Set 2.10 Change Model to Random Forest 2.11 Build Model on all Training and Validation Data Using the Best Model 2.12 Make Predictions with New Data", " 2 Modeling in R With Tidymodels The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org/ Many modeling techniques in R require different syntaxes and different data structures. Tidymodels provides a modeling workflow that standardizes syntaxes and data structures regardless of the model type. lm() glm() glmnet() randomForest() xgboost() c(&quot;linear_reg&quot;, &quot;logistic_reg&quot;, &quot;surv_reg&quot;, &quot;multinom_reg&quot;, &quot;rand_forest&quot;, &quot;boost_tree&quot;, &quot;svm_poly&quot;, &quot;decision_tree&quot;) %&gt;% map_dfr(.f = ~show_engines(x = .x) %&gt;% mutate(type = .x)) %&gt;% DT::datatable() 2.1 Tidymodels Packages Like the tidyverse, tidymodels is a meta package consisting of the following packages: {rsample}: Creates different types of resamples and corresponding classes for analysis {recipes}: Uses dplyr-like pipeable sequences of feature engineering steps to get data ready for modeling {workflows}: Creates an object that can bundle together your pre-processing, modeling, and post-processing steps {parsnip}: Provides a tidy, unified interface to models than can by used to try a range of models without getting bogged down in the syntactical minutiae of the underlying packages {tune}: Facilitates hyperparameter tuning for the tidymodels packages {yardstick}: Estimates how well models are working using tidy data principles {infer}: Performs statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework 2.1.1 Tidymodels Road Map What we plan to do: Explore data {dplyr} Manipulate data {ggplot2} Visualize data Create model {rsample} Split data into test/train {recipes} Preprocess data {parsnip} Specify model {workflows} Create workflow {tune} / {dials} Train and tune parameters {parsnip} Finalize model {yardstick} Validate model Predict on new data 2.1.2 Modeling Goal We would like to create a model to predict which future Major League Baseball players will make the Hall of Fame. We will use historical data to build a model and then use that model to predict who may make the Hall of Fame from the players in the eligible data. 2.2 Explore Data library(tidyverse) historical &lt;- read_csv(&quot;01_data/historical_baseball.csv&quot;) %&gt;% mutate(inducted = fct_rev(as.factor(inducted))) %&gt;% filter(ab &gt; 250) eligible &lt;- read_csv(&quot;01_data/eligible_baseball.csv&quot;) historical ## # A tibble: 2,664 x 15 ## player_id inducted g ab r h x2b x3b hr rbi sb ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 1 3298 12364 2174 3771 624 98 755 2297 240 ## 2 aaronto01 0 437 944 102 216 42 6 13 94 9 ## 3 abbated01 0 855 3044 355 772 99 43 11 324 142 ## 4 adairje01 0 1165 4019 378 1022 163 19 57 366 29 ## 5 adamsba01 0 482 1019 79 216 31 15 3 75 1 ## 6 adamsbe01 0 267 678 37 137 17 4 2 45 9 ## 7 adamsbo03 0 1281 4019 591 1082 188 49 37 303 67 ## 8 adamssp01 0 1424 5557 844 1588 249 48 9 394 154 ## 9 adcocjo01 0 1959 6606 823 1832 295 35 336 1122 20 ## 10 ageeto01 0 1129 3912 558 999 170 27 130 433 167 ## # ... with 2,654 more rows, and 4 more variables: cs &lt;dbl&gt;, bb &lt;dbl&gt;, so &lt;dbl&gt;, ## # last_year &lt;dbl&gt; The historical data contains career statistics for every baseball batter from 1880-2011 who no longer meets Hall of Fame eligibility requirements or has already made the Hall of Fame. Hall of Fame Qualifications: Played at least 10 years Retired for at least 5 years Players have only 10 years of eligibility The eligible data contains everyone who is still eligible for the Hall of Fame. You can see from the data below, the players who make the Hall of Fame tend to perform better in a few standard baseball statistics. historical %&gt;% select(-last_year) %&gt;% group_by(inducted) %&gt;% summarise(across(.cols = where(is.numeric), .fns = ~round(mean(.),0))) %&gt;% gt::gt() ## renders the table html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gvmnnvaysp .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gvmnnvaysp .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gvmnnvaysp .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gvmnnvaysp .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #gvmnnvaysp .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gvmnnvaysp .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gvmnnvaysp .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gvmnnvaysp .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gvmnnvaysp .gt_column_spanner_outer:first-child { padding-left: 0; } #gvmnnvaysp .gt_column_spanner_outer:last-child { padding-right: 0; } #gvmnnvaysp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #gvmnnvaysp .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #gvmnnvaysp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gvmnnvaysp .gt_from_md > :first-child { margin-top: 0; } #gvmnnvaysp .gt_from_md > :last-child { margin-bottom: 0; } #gvmnnvaysp .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gvmnnvaysp .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #gvmnnvaysp .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gvmnnvaysp .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #gvmnnvaysp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gvmnnvaysp .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gvmnnvaysp .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gvmnnvaysp .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gvmnnvaysp .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gvmnnvaysp .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #gvmnnvaysp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gvmnnvaysp .gt_sourcenote { font-size: 90%; padding: 4px; } #gvmnnvaysp .gt_left { text-align: left; } #gvmnnvaysp .gt_center { text-align: center; } #gvmnnvaysp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gvmnnvaysp .gt_font_normal { font-weight: normal; } #gvmnnvaysp .gt_font_bold { font-weight: bold; } #gvmnnvaysp .gt_font_italic { font-style: italic; } #gvmnnvaysp .gt_super { font-size: 65%; } #gvmnnvaysp .gt_footnote_marks { font-style: italic; font-size: 65%; } inducted g ab r h x2b x3b hr rbi sb cs bb so 1 1675 5941 958 1747 295 77 149 874 165 37 643 564 0 907 2849 374 751 122 28 50 336 60 20 264 325 The plot of the data supports this as well. historical %&gt;% pivot_longer(g:so) %&gt;% ggplot(aes(x = inducted, y = value)) + geom_boxplot() + facet_wrap(~name, scales = &quot;free&quot;) + labs(y = &quot;&quot;,x = &quot;Hall of Fame Indicator&quot;) Of note, we are dealing with imbalance classes which will take unique considerations. To have a quality model, we hope to achieve greater than ~93% accuracy since this is what we could do by simply saying that no one should be in the Hall of Fame. historical %&gt;% count(inducted) %&gt;% mutate(Percent = str_c(round(n / sum(n),4)*100,&quot;%&quot;)) %&gt;% gt::gt() ## renders the table html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #etykvszkhs .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #etykvszkhs .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #etykvszkhs .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #etykvszkhs .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #etykvszkhs .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #etykvszkhs .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #etykvszkhs .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #etykvszkhs .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #etykvszkhs .gt_column_spanner_outer:first-child { padding-left: 0; } #etykvszkhs .gt_column_spanner_outer:last-child { padding-right: 0; } #etykvszkhs .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #etykvszkhs .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #etykvszkhs .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #etykvszkhs .gt_from_md > :first-child { margin-top: 0; } #etykvszkhs .gt_from_md > :last-child { margin-bottom: 0; } #etykvszkhs .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #etykvszkhs .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #etykvszkhs .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #etykvszkhs .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #etykvszkhs .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #etykvszkhs .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #etykvszkhs .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #etykvszkhs .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #etykvszkhs .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #etykvszkhs .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #etykvszkhs .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #etykvszkhs .gt_sourcenote { font-size: 90%; padding: 4px; } #etykvszkhs .gt_left { text-align: left; } #etykvszkhs .gt_center { text-align: center; } #etykvszkhs .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #etykvszkhs .gt_font_normal { font-weight: normal; } #etykvszkhs .gt_font_bold { font-weight: bold; } #etykvszkhs .gt_font_italic { font-style: italic; } #etykvszkhs .gt_super { font-size: 65%; } #etykvszkhs .gt_footnote_marks { font-style: italic; font-size: 65%; } inducted n Percent 1 232 8.71% 0 2432 91.29% 2.3 Split Data To begin the analysis, we will load the {tidymodels} library. library(tidymodels) We will split the data into a training (2/3s of the data) and testing set (1/3) of the data. We set the seed so the analysis is reproducible. The output of this function is an rsplit object. An rsplit object is one that can be used with the training and testing functions to extract the data in each split. set.seed(42) data_split &lt;- initial_split(historical, prop = 2/3, strata = inducted) data_split ## &lt;Analysis/Assess/Total&gt; ## &lt;1776/888/2664&gt; We can extract the data from the rsplit object. train_data &lt;- training(data_split) test_data &lt;- testing(data_split) train_data ## # A tibble: 1,776 x 15 ## player_id inducted g ab r h x2b x3b hr rbi sb ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 1 3298 12364 2174 3771 624 98 755 2297 240 ## 2 aaronto01 0 437 944 102 216 42 6 13 94 9 ## 3 abbated01 0 855 3044 355 772 99 43 11 324 142 ## 4 adairje01 0 1165 4019 378 1022 163 19 57 366 29 ## 5 adamsbe01 0 267 678 37 137 17 4 2 45 9 ## 6 adamssp01 0 1424 5557 844 1588 249 48 9 394 154 ## 7 ageeto01 0 1129 3912 558 999 170 27 130 433 167 ## 8 aguaylu01 0 568 1104 142 260 43 10 37 109 7 ## 9 aguirha01 0 447 388 14 33 7 1 0 21 1 ## 10 ainsmed01 0 1078 3048 299 707 108 54 22 317 86 ## # ... with 1,766 more rows, and 4 more variables: cs &lt;dbl&gt;, bb &lt;dbl&gt;, so &lt;dbl&gt;, ## # last_year &lt;dbl&gt; From the training data, we further split the data into a training set (two-thirds of the training data) and a validation set (one-third of the training data) for parameter tuning and model assessment. set.seed(42) validation_set &lt;- validation_split(data = train_data, prop = 2/3, strata = inducted) validation_set ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1184/592]&gt; validation 2.4 Prepare Data What preprocessing steps do you want to do to your data every time you model? We need to specify the following things: Specify the modeling formula Specify the roles of each of the factors Do all preprocessing steps In the {tidymodels} construct, we do this by creating a recipe. baseball_recipe &lt;- recipe(inducted ~ ., data = train_data) %&gt;% update_role(player_id, new_role = &quot;ID&quot;) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_nzv(all_numeric()) %&gt;% step_rm(&quot;last_year&quot;) baseball_recipe ## Data Recipe ## ## Inputs: ## ## role #variables ## ID 1 ## outcome 1 ## predictor 13 ## ## Operations: ## ## Centering for all_numeric() ## Scaling for all_numeric() ## Sparse, unbalanced variable filter on all_numeric() ## Delete terms &quot;last_year&quot; 2.5 Specify Model Now that weve prepared our data, we need to specify the model we wish to execute. Here we identify the model type, specify parameters which need tuning, and then set our desired engine (essentially, the modeling algorithm). lr_mod &lt;- logistic_reg(mode = &quot;classification&quot;, penalty = tune(), mixture = 1) %&gt;% set_engine(engine = &quot;glmnet&quot;) lr_mod ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet 2.6 Create Workflow Now that weve prepared the data and specified the model, we put it all together in a workflow. In a workflow, we add the specified model and the preprocessing recipe. baseball_workflow &lt;- workflow() %&gt;% add_model(lr_mod) %&gt;% add_recipe(baseball_recipe) baseball_workflow ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: logistic_reg() ## ## -- Preprocessor ---------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model ----------------------------------------------------------------------- ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet 2.7 Specify Grid of Training Parameters This step not only executes the model building procedure, but also tunes the penalty hyperparameter by running the model with every penalty option in a specified search grid. First we specify the parameters and search grids that well use for tuning. lr_reg_grid &lt;- tibble(penalty = 10^seq(-4, -1, length.out = 30)) lr_reg_grid ## # A tibble: 30 x 1 ## penalty ## &lt;dbl&gt; ## 1 0.0001 ## 2 0.000127 ## 3 0.000161 ## 4 0.000204 ## 5 0.000259 ## 6 0.000329 ## 7 0.000418 ## 8 0.000530 ## 9 0.000672 ## 10 0.000853 ## # ... with 20 more rows 2.8 Train Model Next, we use tune_grid() to execute the model one time for each parameter set. In this instance, this is 30 times. This function has several arguments: grid: The tibble we created that contains the parameters we have specified. control: Controls various aspects of the grid search process. metrics: Specifies the model quality metrics we wish to save for each model in cross validation. We also specify that we wish to save the performance metrics for each of the 30 iterations. set.seed(42) lr_validation &lt;- baseball_workflow %&gt;% tune_grid(validation_set, grid = lr_reg_grid, control = control_grid(save_pred = TRUE, verbose = TRUE, allow_par = FALSE), metrics = metric_set(roc_auc, accuracy)) lr_validation ## # Tuning results ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [1184/592]&gt; validation &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [17,~ Here, we extract out the best 25 models based on accuracy and plot them vs the penalty from the tuning parameter grid. lr_validation %&gt;% show_best(&quot;accuracy&quot;, n = 25) %&gt;% arrange(penalty) %&gt;% as.data.frame() %&gt;% ggplot(aes(x = penalty, y = mean)) + geom_point() + geom_line() + scale_x_log10() We want the simplest possible model that retains very high accuracy. Because of that, we select the 7th smallest penalty. lr_best &lt;- lr_validation %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% slice(7) We show the ROC curve for the selected model. lr_validation %&gt;% collect_predictions(parameters = lr_best) %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.9 Build Model on all Training Data, Test on Validation Set Now that weve found the best parameter set, we need to apply this model to the entire training set. Well make one tweak to our previous model specification: we specify our chosen penalty from the tuning process, instead of allowing the penalty to be tuned automatically. last_lr_mod &lt;- logistic_reg(mode = &quot;classification&quot;, penalty = lr_best$penalty, mixture = 1) %&gt;% set_engine(engine = &quot;glmnet&quot;) last_lr_mod ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = lr_best$penalty ## mixture = 1 ## ## Computational engine: glmnet We update our workflow to have the best parameter set with the function finalize_workflow(). last_lr_workflow &lt;- baseball_workflow %&gt;% finalize_workflow(lr_best) last_lr_workflow ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: logistic_reg() ## ## -- Preprocessor ---------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model ----------------------------------------------------------------------- ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0.00041753189365604 ## mixture = 1 ## ## Computational engine: glmnet We fit the model on the entire training set. last_lr_fit &lt;- last_lr_workflow %&gt;% last_fit(data_split) We can see the performance of the model below in the next two outputs. last_lr_fit %&gt;% collect_metrics() ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.922 Preprocessor1_Model1 ## 2 roc_auc binary 0.767 Preprocessor1_Model1 last_lr_workflow %&gt;% fit(data = historical) %&gt;% predict(historical, type = &quot;prob&quot;) %&gt;% bind_cols(historical) %&gt;% mutate(pred_class = fct_rev(as.factor(round(.pred_1)))) %&gt;% conf_mat(inducted, pred_class) ## Truth ## Prediction 1 0 ## 1 76 11 ## 0 156 2421 And we can take a view look at the ROC curve of our final model. last_lr_fit %&gt;% collect_predictions() %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.10 Change Model to Random Forest 2.10.1 Update Model Type rf_mod &lt;- rand_forest(mode = &quot;classification&quot;, mtry = tune(), min_n = tune(), trees = tune()) %&gt;% set_engine(engine = &quot;randomForest&quot;) rf_mod ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = tune() ## min_n = tune() ## ## Computational engine: randomForest 2.10.2 Update Workflow baseball_workflow_rf &lt;- workflow() %&gt;% add_model(rf_mod) %&gt;% add_recipe(baseball_recipe) baseball_workflow_rf ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: rand_forest() ## ## -- Preprocessor ---------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model ----------------------------------------------------------------------- ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = tune() ## min_n = tune() ## ## Computational engine: randomForest 2.10.3 Update Tuning Parameters rf_reg_grid &lt;- dials::grid_latin_hypercube(mtry(c(1,10)), min_n(), trees(), size = 30) rf_reg_grid ## # A tibble: 30 x 3 ## mtry min_n trees ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 7 36 1267 ## 2 7 4 111 ## 3 9 5 1244 ## 4 2 27 795 ## 5 8 13 1934 ## 6 4 24 832 ## 7 2 22 1795 ## 8 10 16 199 ## 9 6 10 537 ## 10 6 7 40 ## # ... with 20 more rows 2.10.4 Re Execute Cross Validation set.seed(42) rf_validation &lt;- baseball_workflow_rf %&gt;% tune_grid(validation_set, grid = rf_reg_grid, control = control_grid(save_pred = TRUE, verbose = TRUE, allow_par = FALSE), metrics = metric_set(roc_auc, accuracy)) rf_validation ## # Tuning results ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [1184/592]&gt; validation &lt;tibble [60 x 7]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [17,~ 2.10.5 Explore Tuning Parameters rf_validation %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% pivot_longer(cols = mtry:min_n) %&gt;% mutate(best_mod = mean == max(mean)) %&gt;% ggplot(aes(x = value, y = mean)) + # geom_line(alpha = 0.5, size = 1.5) + geom_point(aes(color = best_mod)) + facet_wrap(~name, scales = &quot;free_x&quot;) + scale_x_continuous(breaks = scales::pretty_breaks()) + labs(y = &quot;Accuracy&quot;, x = &quot;&quot;, color = &quot;Best Model&quot;, title = &quot;Random Forest Cross Validation Tuning Parameters&quot;) 2.10.6 Select Best Tuning Parameters for Random Forest rf_best &lt;- rf_validation %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% arrange(desc(mean)) %&gt;% slice(1) 2.10.7 Show ROC Curve for Best Random Forest Model rf_validation %&gt;% collect_predictions(parameters = rf_best) %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() last_rf_mod &lt;- rand_forest(mode = &quot;classification&quot;, mtry = rf_best$mtry, trees = rf_best$trees, min_n = rf_best$min_n) %&gt;% set_engine(engine = &quot;randomForest&quot;) last_rf_mod ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = rf_best$mtry ## trees = rf_best$trees ## min_n = rf_best$min_n ## ## Computational engine: randomForest We update our workflow to have the best parameter set with the function finalize_workflow(). last_rf_workflow &lt;- baseball_workflow_rf %&gt;% finalize_workflow(rf_best) last_rf_workflow ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: rand_forest() ## ## -- Preprocessor ---------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model ----------------------------------------------------------------------- ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = 4 ## trees = 1034 ## min_n = 6 ## ## Computational engine: randomForest We fit the model on the entire training set. last_rf_fit &lt;- last_rf_workflow %&gt;% last_fit(data_split) We can see the performance of the model below in the next two outputs. last_rf_fit %&gt;% collect_metrics() ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.934 Preprocessor1_Model1 ## 2 roc_auc binary 0.909 Preprocessor1_Model1 last_rf_workflow %&gt;% fit(data = historical) %&gt;% predict(historical, type = &quot;prob&quot;) %&gt;% bind_cols(historical) %&gt;% mutate(pred_class = fct_rev(as.factor(round(.pred_1)))) %&gt;% conf_mat(inducted, pred_class) ## Truth ## Prediction 1 0 ## 1 189 2 ## 0 43 2430 And we can take a view look at the ROC curve of our final model. last_lr_fit %&gt;% collect_predictions() %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.11 Build Model on all Training and Validation Data Using the Best Model Now, we can use the fit() function to build the model on the entire historical data. last_rf_workflow %&gt;% fit(data = historical) %&gt;% extract_fit_parsnip() ## parsnip model object ## ## Fit time: 4.9s ## ## Call: ## randomForest(x = maybe_data_frame(x), y = y, ntree = ~1034L, mtry = min_cols(~4L, x), nodesize = min_rows(~6L, x)) ## Type of random forest: classification ## Number of trees: 1034 ## No. of variables tried at each split: 4 ## ## OOB estimate of error rate: 5.78% ## Confusion matrix: ## 1 0 class.error ## 1 103 129 0.55603448 ## 0 25 2407 0.01027961 2.12 Make Predictions with New Data Now that we have the model, we can make predictions on the eligible data. How did we do? last_rf_workflow %&gt;% fit(data = historical) %&gt;% predict(eligible, type = &quot;prob&quot;) %&gt;% bind_cols(eligible) %&gt;% arrange(-.pred_1) %&gt;% filter(.pred_1 &gt;.4) %&gt;% mutate(across(contains(&quot;pred&quot;), ~round(.,3))) %&gt;% # print(n = Inf) %&gt;% DT::datatable() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
