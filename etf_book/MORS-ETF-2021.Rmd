---
title: "Modeling in R and Python"
author: "MAJ Dusty Turner and Robert Ward"
date: "7 DEC 2021"
output: bookdown::gitbook
# output:
#   bookdown::html_book:
#     theme: united
site: bookdown::bookdown_site
always_allow_html: yes
documentclass: memoir
classoption: oneside
# geometry: margin=.75in
---

```{r setup, include=FALSE, cache = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
```

# Class Introduction

<center>
![](img/caa_seal.png){ width=25% }
</center>


**Disclaimer:**  The appearance of U.S. Department of Defense (DoD) visual information does not imply or constitute DoD endorsement.  The views expressed in this presentation are those only of the author and do not represent the official position of the U.S. Army, DoD, or the federal government.

#### need to add the following things
1. link for this book
2. do we need to have students pip install anything

## Topics & Class Structure

1. Overview of modeling
2. Tidymodels (R)  
3. scikit-learn (Python)

## Software Prerequisites

1. R 3.6.x or newer
2. RStudio 1.2.x or newer
3. Python (version x)

## Human Prerequisites

We assume you have:

1. A working knowledge of R and RStudio and/or Python;
2. Some experience with contemporary tidy coding concepts;
3. An understanding of modeling principals. 

Do your best to follow along.  We are happy to answer questions.  This presentation is available at [this link](www.google.com)

## Distance Learning Challenges

1. Don't be afraid to ask questions - both verbally and in chat.
2. If you miss something we said, it's likely others have too - you'll be helping them by speaking up.
3. Its difficult to know how we should pace the class, so please communicate!

## Endstate

1. Students generally understand the modeling process in R and Python;
2. Students have access to resources to learn more.

## Instructors Introduction

### MAJ Dusty Turner

Army 

- Combat Engineer
- Platoon Leader / XO / Company Commander
- Geospatial / Sapper / Route Clearance
- Hawaii / White Sands Missile Range / Iraq / Afghanistan

Education

- West Point '07
  - Operations Research, BS
- Missouri University of Science and Technology '12
  - Engineering Management, MS
- THE Ohio State '16
  - Integrated Systems Engineering, MS
  - Applied Statistics, Graduate Minor

Data Science

- R User Since '14
- Catch me on Twitter [`@dtdusty`](http://www.twitter.com/dtdusty)
- <http://dustysturner.com/>

### Robert Ward

Education

- University of Chicago, '13
  - Political Science & English, BA
- Columbia University School of International and Public Affairs, '18
  - Master of International Affairs, Specialization in Advanced Policy and Economic Analysis
  
Data Science

- R user since 2011; also know some python and forgot some Stata
- Worked for Government Accountability Office Applied Research & Methods
- Operations Research Systems Analyst at CAA and Army Leader Dashboard/Vantage PM team


## Lets Get Started...

### Prerequisite Packages

```{r eval=FALSE, include=TRUE}
install.packages(c("tidyverse", "tidymodels"), dependencies = TRUE)
```

```{python eval = FALSE, include = TRUE}
pip install something 
```


<!--chapter:end:index.Rmd-->


<!-- --- -->
<!-- title: "Tidymodels" -->
<!-- author: "Dusty and Robert" -->
<!-- date: "7/21/2021" -->
<!-- output: html_document -->
<!-- --- -->


<!-- ```{r setup, include=FALSE} -->
<!-- knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE) -->
<!-- knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) -->

<!-- ``` -->

# Modeling in R With Tidymodels

"The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles."

[https://www.tidymodels.org/](https://www.tidymodels.org/)

Many modeling techniques in R require different syntax and different data structures.  Tidymodels provides modeling workflow that standardizes syntax and data structures regardless of the model type.  

## Tidymodels Packages

Like the tidyverse, tidymodels is a 'meta package' consisting of the following packages:

- {[rsample](https://rsample.tidymodels.org/)}:  Creates different types of resamples and corresponding classes for analysis
- {[recipes](https://recipes.tidymodels.org/)}:  Uses dplyr-like pipeable sequences of feature engineering steps to get data ready for modeling
- {[workflows](https://workflows.tidymodels.org/index.html)}:   Creates an object that can bundle together your pre-processing, modeling, and post-processing steps
- {[parsnip](https://parsnip.tidymodels.org/)}: Provides a tidy, unified interface to models than can by used to try a range of models without getting bogged down in the syntactical ninutae of the underslying packages.
- {[tune](https://tune.tidymodels.org/)}: Facilitates hyperparameter tuning for the tidymodels packages.
- {[yardstick](https://yardstick.tidymodels.org/index.html)}: Estimates how well models are working using tidy data principles.
- {[infer](https://infer.tidymodels.org/index.html)}: Performs statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework.

### Tidymodels Road Map

What we plan to do:

1. Explore data
2. Create model
   - {rsample} Split data into test/train
   - {recipies} Preprocess data
   - {parsnip} Specify model
   - {workflows} Create workflow
   - {tune} / {dials} Train and tune parameters
   - {parsnip} Finalize model
   - {yardstick} Validate model
3. Predict on new data

### Modeling Goal

We desire to create a model using the `historical` data and use that model to predict who may make the Hall-of-Fame in the `eligible` data.

## Explore Data

```{r}
library(tidyverse)

historical <- read_csv("01_data/historical_baseball.csv") %>% 
  mutate(inducted = as.factor(inducted))
eligible <- read_csv("01_data/eligible_baseball.csv")

historical
```

The `historical` data contains career statistics of every baseball batter from 1880-2011 who no longer meet Hall-of-Fame eligability requirements or who have already made the hall-of-fame.

Hall of Fame Qualifications  
- Play at least 10 years  
- Retired for at least 5 years  
- Players have only 10 years of eligability   

The `eligible` data contains everyone who is still eligible for the Hall-of-Fame

You can see from the data below, the players who make the Hall-of-Fame tend to perform better in a few standard baseball statistics. 

```{r}
historical %>%
  select(-last_year) %>% 
  group_by(inducted) %>% 
  summarise(across(.cols = where(is.numeric), .fns = ~round(mean(.),0))) %>% 
  gt::gt()
```

The plot of the data supports this as well. 

```{r}
historical %>% 
  pivot_longer(g:so) %>% 
  ggplot(aes(x = inducted, y = value)) +
  geom_boxplot() +
  facet_wrap(~name, scales = "free")  +
  labs(y = "",x = "Hall-of-Fame Indicator")
```

## Split Data

To begin the analysis, we will load the {tidymodels} library.

```{r}
library(tidymodels)
```

We will split the data into a training (2/3s of the data) and testing set (1/3) of the data.

We set the seed so the analysis is reproducible.  

The output of this function is an rsplit object.  An rsplit object that can be used with the training and testing functions to extract the data in each split.

```{r}
set.seed(42)

data_split <- initial_split(historical, prop = 2/3, strata = inducted)

data_split
```

We can extract the data from the rsplit object.

```{r}
train_data <- training(data_split)
test_data  <- testing(data_split)

train_data
```

From the training data, we further split the data into a training (2/3 of the training data) and a validation set (1/3 of the training data) for parameter tuning and model assessment.

```{r}
set.seed(42)

validation_set <- validation_split(data = train_data, prop = 2/3, strata = inducted)
validation_set
```

## Prepare Data

What preprocessing steps do you want to do to your date every time you model?  

We need to specify the following things:
- Specify the modeling formula  
- Specify the 'roles' of each of the factors  
- Do all preprocessing steps  

In the {tidymodels} construct, we do this by creating a recipe.

```{r}

baseball_recipie <-
  recipe(inducted ~ ., data = train_data) %>% 
  update_role(player_id, new_role = "ID") %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>% 
  step_nzv(all_numeric()) %>% 
  step_rm("last_year")

baseball_recipie
```

## Specify Model

Now that we've prepared our data, we need to specify the model we wish to execute.

Here we identify the model type, specify parameters which need tuning, and then set the 'engine' (package) we wish to do the work.

```{r}
lr_mod <-
  logistic_reg(mode = "classification", penalty = tune(), mixture = 1) %>% 
  set_engine(engine = "glmnet")

lr_mod
```

## Create Workflow

Now that we've prepared the data and specified the model, we put it all together in a workflow.

In a workflow, we add the specified model and the preprocessing recipe.

```{r}
baseball_workflow <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(baseball_recipie)

baseball_workflow
```

## Train Model and Tune Parameters

This step not only executes the model building proceedure, but it contains the structure to execute the model over a grid of parameters to tune the model and find the best parameter set.

First we specify the parameters over which we desire to tune.

```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
```

Next, we use `tune_grid()` to execute the model one time for each parameter set.  In this instance, this is 30 times.

This function has several arguments

`grid`: The tibble we created that contains the parameters we have specified.
`control`: Controls the apects of the grid search process.  
`metrics`: Specifies the model quality metrics we wish to save for each model in cross validation.

We also specify that we wish to save the performance metrics for each of the 30 iterations.  

```{r}
set.seed(42)

lr_validation <-
  baseball_workflow %>% 
  tune_grid(validation_set,
            grid = lr_reg_grid, 
            control = control_grid(save_pred = TRUE, 
                                   verbose = TRUE, 
                                   allow_par = FALSE),
            metrics = metric_set(roc_auc, accuracy))

lr_validation
```


Here, we extract out the best 25 models based on accuracy and plot them vs the penalty from the tuning parameter grid.

```{r}
lr_validation %>% 
  show_best("accuracy", n = 25) %>% 
  arrange(penalty) %>% as.data.frame() %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10()
```

We desire the penalty that gives us the simplest model, with the best accuracy.  Because of that, we select the 7th smallest penalty.

```{r}
lr_best <-
  lr_validation %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  slice(7)
```

We show the ROC curve for the selected model.

```{r}
lr_validation %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()

```


## Build model on all training data, test on validation set.

Now that we've found the best parameter set, we need to apply this model to the entire training dataset.

We made one tweak to our model specification.  We specify the specific penalty from our best model from cross validation. 

```{r}
last_lr_mod <-
  logistic_reg(mode = "classification", penalty = lr_best$penalty, mixture = 1) %>% 
  set_engine(engine = "glmnet")

last_lr_mod
```

We update our workflow to have the best parameter set with the function `finalize_workflow()`.

```{r}
last_lr_workflow <-
  baseball_workflow %>%
  finalize_workflow(lr_best)

last_lr_workflow
```

We fit the model on the entire training set.

```{r}
last_lr_fit <-
  last_lr_workflow %>% 
  last_fit(data_split)
```

We can see the performance of the model below.

```{r}
last_lr_fit %>% 
  collect_metrics()
```

And we can take a view look at the ROC of our final model.

```{r}
last_lr_fit %>% 
  collect_predictions() %>% 
  roc_curve(inducted, .pred_1) %>% 
  autoplot()
```

## Build model on all training and validation data

Now, we can use the `fit()` function to build the model on the entire `historical` data. 

```{r}
last_lr_workflow %>% 
  fit(data = historical) %>%
  pull_workflow_fit() %>% tidy()
```

Now that we have the model, we can make predictions on the `eligible` data.

How did we do?


```{r}
last_lr_workflow %>% 
  fit(data = historical) %>%
  predict(eligible, type = "prob") %>% 
  bind_cols(eligible) %>% 
  arrange(-.pred_1) %>% 
  filter(.pred_1 >.4) %>%
  mutate(across(contains("pred"), ~round(.,3))) %>% 
  # print(n = Inf) %>% 
  DT::datatable()
```


<!--chapter:end:1-Tidymodels.Rmd-->

# Modeling in python with scikit-learn

## Scikit-learn Overview

Like {tidymodels}, scikit-learn offers a suite of tools for predictive modeling and machine learning: it will help you split data, preprocess model inputs, fit models, and compare and assess them.

Unlike {tidymodels}, scikit-learn is a single monolithic package with functions for the entire modeling pipeline. Users will likely still want to use pandas to ingest and prepare data, and may want to use other libraries to supplement scikit-learn's data visualiation capabilities, but scikit-learn will do most of the work by itself - and often with less and simpler code than tidymodels, at least for standard machine learning workflows.

### Tidymodels Road Map

What we plan to do:

1. Read in and explore data (pandas and R)
2. Create model (scikit-learn)
   - split data
   - define pipeline with preprocessors and model with cross-validation for parameter tuning
   - fit model
3. Predict on new data and assess model (scikit-learn)

### Modeling Goal

We plan to create a model using the `historical` data and use that model to predict who is most likely to make it into the Hall of Fame in the `eligible` data.

## Explore Data

```{r include=FALSE}
library(reticulate)
library(tidyverse)
```
We'll load the pandas library to import and set up the data.

```{python}
import pandas as pd
import numpy as np
```
Here, we use panda's `read_csv()` to import the data, and then we print the first few rows of the historical dataframe to the console.

```{python}
historical = pd.read_csv('01_data/historical_baseball.csv')
eligible = pd.read_csv('01_data/eligible_baseball.csv')

historical
```
The `historical` data contains career statistics of every baseball batter from 1880-2011 who no longer meet Hall-of-Fame eligibility requirements or who have already made the hall-of-fame.

Hall of Fame Qualifications  
- Play at least 10 years  
- Retired for at least 5 years  
- Players have only 10 years of eligability   

The `eligible` data contains everyone who is still eligible for the Hall-of-Fame

You can see from the data below, the players who make the Hall-of-Fame tend to perform better in a few standard baseball statistics. This pandas code does the same thing as the R code in the previous chapter - it groups the historical data by whether or not the player was inducted into the Hall of Fame, and then takes the mean of each column in each group.

```{python}
hist_means_inducted_groups = historical.drop('last_year', axis = 1) \
  .groupby('inducted') \
  .mean() \
  .round()
```
We bring the data back into R, using RStudio's very simple Python-R interface, and use the same R code as in Chapter 1 to print a table and produce boxplots of the means.

```{r}
py$hist_means_inducted_groups %>%
  rownames_to_column(var = "inducted") %>%
  gt::gt()
```

```{r}
historical %>% 
  pivot_longer(g:so) %>% 
  ggplot(aes(x = inducted, y = value)) +
  geom_boxplot() +
  facet_wrap(~name, scales = "free")  +
  labs(y = "",x = "Hall-of-Fame Indicator")
```

## Split Data

As we did in R, we will split the data into a training set (2/3s of the data) and testing set (1/3) of the data.

We set the seed so the analysis is reproducible - here, we do this using the `random_state` parameter in `train_test_split()`.

Instead of an `rsplit` object that contains resampling metadata, `train_test_split()` returns four objects: X (predictor) pandas DataFrame objects for the training and test sets, and y (target) pandas Series objects for the training and test sets.

Note that before splitting the data, we set the index of the dataframe to be `player_id`. This carries through to the outputs of `train_test_split()`, which all have `player_id` as a common index (and not as a predictor or target variable.) In a way, this serves a similar purpose to the `update_role(player_id, new_role = "ID")` line that we added to the recipe in R.

```{python}
from sklearn.model_selection import train_test_split

historical_pidindex = historical.set_index('player_id')

X = historical_pidindex.drop(['inducted', 'last_year'], axis = 1)
y = historical_pidindex.inducted

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size = 1/3)
```

## Define a Pipeline

Scikit-learn's "pipelines" serve the combined purpose of "workflows" and "recipes" in {tidymodels}. They allow you to define a set of preprocessing and modeling steps that you can then apply to any dataset. They are defined by the function `make_pipeline()`, with the steps, in order, as arguments.

The first two steps in our pipeline will take care of preprocessing. In the previous chapter, we centered and scaled our data; here, we'll use `StandardScaler()`, which accomplishes both of those steps. We'll also apply `VarianceThreshold()`; in its default form, this only removes zero-variance predictors, but the user can set a custom variance threshold. None of our predictors have low variance, so this feature selection mechanism does nothing anyway.

The third step in our pipeline is our model. Here, we've chosen `LogisticRegressionCV()`. The first three parameters should produce a model very similar to the one in the previous chapter:
  - `Cs = 10`: the modeling function will automatically select a grid of 10 C values (inverse penalties) to search over. This is the default value. The user can also specify a specific list of C values to search over.
  - `penalty = "elasticnet"` lets us use a hybrid L1 and L2 penalty, or a mix between Lasso and Ridge regression, much like `engine = glmnet` in R;
  - `solver = "saga"` chooses a solver that is compatible with our other options;
  - `l1_ratios = [1.0]` is the equivalent of `mixture = 1` in R - it gives us a pure Lasso regression;
  - `max_iter = 1000` allows the solver to attempt up to 1000 iterations as it searches for a solution. The default of 100 was insufficient for this data. 
  - `refit = True`: the function will find the best C (inverse penalty) value by averaging the cross-validation scores of each one, and then refit the model using the best C value on all of the data.

We also have one parameter related to the cross-validation (CV) part of the model specification: `cv = 10`. This means that the data will be split into ten folds, and the model will be fit ten times for each set of hyperparameters in an automatically generated search grid, with one fold being held out as a validation set for computing accuracy in each run. This process will allow the model to tune the size of penalty, which we have not specified explicitly.

Finally, we set `n_jobs = 4` to allow for multithreading. In my own highly unscientific testing, moving from 1 to 4 threads reduces model fit time from 15 seconds to 6 seconds.

```{python}
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold

# l1_ratio

pipe_scale_lr_lasso = make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegressionCV(Cs = 10, penalty = "elasticnet", solver = "saga", l1_ratios = [1.0], cv = 10, max_iter = 1000, n_jobs = 4))
```

It is also possible to use a parameter tuning method more like the onein the previous chapter, using `gridsearchCV` and a predefined grid of search values The scikit-learn user guide has a very detailed section on this method, available at: https://scikit-learn.org/stable/modules/grid_search.html

## Fit the Model (Using the Pipeline)

With our pipeline defined, fitting the model on the training data is very easy: we simply call the `fit()` method on the pipeline, with our `X_train` and `y_train` data as the inputs.

```{python}
pipe_scale_lr_lasso.fit(X_train, y_train)  # apply scaling on training data
```
Because we used `LogisticRegressionCV()`, several of the steps we went through more carefully in the first chapter have been done for us: 
  - hyperparameter tuning was done, using an automatically-generated grid of 10 penalty values;
  - the highest-accuracy C value was selected, using the mean scores across all cross-validation runs for each value;
  - the model was refit using all of the data and the highest C value.

In some cases, it may be a better practice not to allow all of these decisions to be made automatically by an algorithm. It is, of course, possible to more precisely replicate the process shown in Chapter 1, by manually selecting a search grid for the penalty value, plotting and evaluating each penalty value, and manually refitting on the training set. For instance, (see here)[https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_refit_callable.html#sphx-glr-auto-examples-model-selection-plot-grid-search-refit-callable-py] for an example of how to "balance model complexity and cross-validated score," which, in this case, means finding a model with the least number of components from principal components analysis while maintaining a good-enough accuracy score.

## Score and Evaluate the Model

### Accuracy and Predictions

The most basic way to assess the performance of a fitted scikit-learn model is the `score()` function, with the test set as inputs. This uses the fitted model to predict on the test set and returns the proportion of correct predictions.

```{python}
pipe_scale_lr_lasso.score(X_test, y_test)
```
We can also get predictions using the `predict()` method, with our `X_test` DataFrame as the sole input. Our modeled predicted that just 

```{python}
y_pred = pipe_scale_lr_lasso.predict(X_test)
```

### Confusion Matrices and Unbalanced Classes

It looks like our model predicted that just 23 out of 1079 players in the test set would be inducted into the hall of fame.

```{python}
pred_series = pd.Series(y_pred)
pred_series.value_counts()
```
While we already know that our model was 94% accurate in the test set, it's also important to compare the predictions versus the actual y_test values with a confusion matrix. In a classification problem with highly unbalanced classes, a prediction model can often score very well by simply predicting the more popular class (here, "not inducted into the Hall of Fame") in nearly every case.

`sklearn.metrics.confusion_matrix()` will produce a confusion matrix as a numpy array; this is useful for further processing, but not especially easy to read. 

```{python}
import sklearn.metrics 

sklearn.metrics.confusion_matrix(y_test, y_pred)
```
Fortunately, scikit-learn will also generate a much prettier and easier-to-understand confusion matrix, with the help of `matplotlib.pyplot`. As expected, our model seriously underpredicted Hall of Fame induction: we had just 4 false positives and 61 false negatives! With 19 true positives, this means that we correctly predicted less than 25% of the Hall of Fame inductees in the test set. 

```{python}
import matplotlib.pyplot as plt

sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
plt.show()
```

### The Decision Boundary, Precision-Recall Curves, and ROC Curves

One possible way to ameliorate this issue is to change the decision boundary. By default, our decision boundary is 0.5: we predict whichever class our model says has a higher probability. However, we might want to lower this threshold, so that we get more true positives - but this will likely come at the cost of having more false positives, as well. One way to assess this tradeoff is the precision-recall curve. Precision the proportion of our positive predictions that were correct; ours was pretty good, at 19/23, or 0.826. Recall is the proportion of positives that we predicted correctly; ours was quite poor, at 19/80, or 0.2375. The precision-recall curve plots precision versus recall at different decision boundaries. Here, we'll mark our current precision and recall, at the 0.5 decision boundary, with red lines.

```{python}
y_score = pipe_scale_lr_lasso.decision_function(X_test)
sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, y_score, name="LogisticRegressionCV")
plt.axhline(y=19/23, color='r', linestyle='-', zorder = -10)
recall_score = sklearn.metrics.recall_score(y_test, y_pred)
plt.axvline(x = recall_score, color = 'r', linestyle = '-', zorder = -10)


plt.show()
```

This plot suggests that we have the option of shifting our decision boundary downward to trade precision for recall. Unfortunately, the tradeoff looks nearly linear - ideally, we would have found that we could gain a lot of recall while only losing a small amount of precision. 

### ROC Curve

Another way to assess our choice of decision boundary, and the model's performance at different boundaries, is the ROC curve, which plots the true positive rate (recall) and the true negative rate (the proportion of negative predictions out of total actual negative values.) We can plot the ROC curve using `RocCurveDisplay` from scikit-learn along with scores from `decision_function()` and the test set labels. Again, we add red lines to show the current decision boundary.

```{python}
y_df = pipe_scale_lr_lasso.decision_function(X_test)
# RocCurveDisplay.from_estimator(pipe_scale_lr_lasso, X_test, y_test)
sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_df)
plt.axhline(y = recall_score, color='r', linestyle='-', zorder = -10) # true positive rate
# recall_score = sklearn.metrics.recall_score(y_test, y_pred)
plt.axvline(x = 4/999, color = 'r', linestyle = '-', zorder = -20)

plt.show()
```
Again, it seems that we could shift the decision boundary downward to get a higher true positive rate - and, in this case, it looks like our false positive rate would barely budge, thanks to the very large number of true negatives in the dataset that we would still be predicting correctly.

Let's shift the decision boundary down to 0.33 and see how it changes our results. *Caveat: tuning your decision boundary on test set predictions is generally a bad idea, because it can easily lead to overfitting to the test set. We're doing it here for simplicity, but you're better off dealing with unbalanced classes while training your model.*

As expected, we added both true and false positives, with a much higher proportional increase in the number of false positives (+275%) than true positives (+63%). This doesn't sound great, but it might be worthwhile if we care much more about detecting actual Hall of Fame inductees than we do about making a few more wrong predictions. 

```{python}
probs = pd.DataFrame(pipe_scale_lr_lasso.predict_proba(X_test), columns = ['prob_zero', 'prob_one'])

preds_onethird = (probs['prob_one'] > 0.33).astype(int)

sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, preds_onethird)
plt.show()
```
Let's also see where this puts us on our Precision-Recall and ROC curves.

```{python}
recall_onethird = sklearn.metrics.recall_score(y_test, preds_onethird)
precision_onethird = 31/46
tnr_onethird = 15/999
```
As expected, we moved to the right on the precision-recall curve, trading precision for recall.

```{python}
# y_score = pipe_scale_lr_lasso.decision_function(X_test)
sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, y_score, name="LogisticRegressionCV")
plt.axhline(y=19/23, color='r', linestyle='-', zorder = -10)
plt.axhline(y=precision_onethird, color='blue', linestyle='-', zorder = -10)
recall_score = sklearn.metrics.recall_score(y_test, y_pred)
plt.axvline(x = recall_score, color = 'r', linestyle = '-', zorder = -10)
plt.axvline(x = recall_onethird, color = 'blue', linestyle = '-', zorder = -10)
plt.show()


```
We also moved to the right on the ROC curve, but just barely! Our true positive rate increased quite a bit more than our false positive rate. Which of these plots and scores we care most about depends on the problem we're trying to solve and our sensitivity to false positives and negatives.

```{python}
# y_df = pipe_scale_lr_lasso.decision_function(X_test)
# RocCurveDisplay.from_estimator(pipe_scale_lr_lasso, X_test, y_test)
sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_df)
plt.axhline(y = recall_score, color='r', linestyle='-', zorder = -10) # true positive rate
plt.axhline(y = recall_onethird, color='blue', linestyle='-', zorder = -10) # true positive rate
# recall_score = sklearn.metrics.recall_score(y_test, y_pred)
plt.axvline(x = 4/999, color = 'r', linestyle = '-', zorder = -20)
plt.axvline(x = tnr_onethird, color = 'blue', linestyle = '-', zorder = -20)


plt.show()

```

<!-- ### Exploring CV Results -->

<!-- ```{python} -->
<!-- pipe_scale_lr_lasso._final_estimator.scores_ -->
<!-- pipe_scale_lr_lasso._final_estimator.Cs_ -->
<!-- pipe_scale_lr_lasso._final_estimator.l1_ratios_ -->
<!-- pipe_scale_lr_lasso._final_estimator.C_ -->

<!-- ``` -->

### Class Weights

If we want to avoid tuning the decision boundary directly, another option is to use the `class_weights` parameter found in many classifiers in scikit-learn. This allows us to increase the penalty for misclassifying the higher-weighted class (here, the less-frequent "inducted into Hall of Fame" class) while fitting the model. There is a "balanced" option for class weights that attempts to fully balance classes by setting class weights inversely proportional to class proportions; unfortunately, our classes are so unbalanced that this method doesn't work on this dataset. Instead, we weight the "positive" class at four times the weight of the "negative" class.

```{python}
pipe_scale_lr_lasso_weighted = make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegressionCV(Cs = 10, penalty = "elasticnet", solver = "saga", l1_ratios = [1.0], cv = 10, max_iter = 3000, n_jobs = 4, class_weight = {0: 0.2, 1: 0.8}))
pipe_scale_lr_lasso_weighted.fit(X_train, y_train)
```

This produces a very small improvement in accuracy.

```{python}
weighted_accuracy = pipe_scale_lr_lasso_weighted.score(X_test, y_test)
print("Weighted accuracy: ", weighted_accuracy)
print("Improvement: ", weighted_accuracy - pipe_scale_lr_lasso.score(X_test, y_test))
```
The difference in the confusion matrix, however, is much more noticeable: our recall is up to nearly 50%, although our precision has decreased as the number of false positives grows.

```{python}
y_pred_weighted = pipe_scale_lr_lasso_weighted.predict(X_test)

sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred_weighted)
plt.show()
```
Our ROC curve looks fairly similar, but the AUC has increased from 0.82 to 0.83, suggesting that this model may be slightly better.

```{python}
y_df_weighted = pipe_scale_lr_lasso_weighted.decision_function(X_test)
# RocCurveDisplay.from_estimator(pipe_scale_lr_lasso, X_test, y_test)
sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_df_weighted)
plt.show()
```

```{python}
sklearn.metrics.PrecisionRecallDisplay.from_predictions(y_test, weighted_accuracy, name="LogisticRegressionCV Weighted")
plt.show()
```


<!--chapter:end:2-sklearn.Rmd-->

