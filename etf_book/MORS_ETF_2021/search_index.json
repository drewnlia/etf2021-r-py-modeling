[["index.html", "Modeling in R and Python 1 Class Introduction 1.1 Topics &amp; Class Structure 1.2 Software Prerequisites 1.3 Human Prerequisites 1.4 Distance Learning Challenges 1.5 Endstate 1.6 Instructors Introduction 1.7 Lets Get Started", " Modeling in R and Python MAJ Dusty Turner and Robert Ward 7 DEC 2021 1 Class Introduction Disclaimer: The appearance of U.S. Department of Defense (DoD) visual information does not imply or constitute DoD endorsement. The views expressed in this presentation are those only of the author and do not represent the official position of the U.S. Army, DoD, or the federal government. 1.1 Topics &amp; Class Structure Overview of modeling Tidymodels (R) scikit-learn (Python) Pros / Cons 1.2 Software Prerequisites R 3.6.x or newer RStudio 1.2.x or newer Python (version x) 1.3 Human Prerequisites We assume you have: Working knowledge of R and RStudio and/or Python; Some experience with contemporary tidy coding concepts; An understanding of modeling principals. Do your best to follow along. We are happy to answer questions. This presentation is available at this link 1.4 Distance Learning Challenges Dont be afraid to ask questions - both verbally and in chat. If you miss something we said, its likely others have too - youll be helping them by speaking up. Its difficult the know the speed of the class, so please communicate! 1.5 Endstate You generally understand the modeling process in R and Python; You have access to resources to learn more. 1.6 Instructors Introduction 1.6.1 MAJ Dusty Turner Army Combat Engineer Platoon Leader / XO / Company Commander Geospatial / Sapper / Route Clearance Hawaii / White Sands Missile Range / Iraq / Afghanistan Education West Point 07 Operations Research, BS Missouri University of Science and Technology 12 Engineering Management, MS THE Ohio State 16 Integrated Systems Engineering, MS Applied Statistics, Graduate Minor Data Science R User Since 14 Catch me on Twitter @dtdusty http://dustysturner.com/ 1.6.2 Robert Ward Education University of Chicago, 13 Political Science &amp; English, BA Columbia University School of International and Public Affairs, 18 Master of International Affairs, Specialization in Advanced Policy and Economic Analysis Data Science R user since 2011; also know some python and forgot some Stata Worked for Government Accountability Office Applied Research &amp; Methods Operations Research Systems Analyst at CAA and Army Leader Dashboard/Vantage PM team 1.7 Lets Get Started 1.7.1 Prerequisite Packages install.packages(c(&quot;tidyverse&quot;, &quot;tidymodels&quot;), dependencies = TRUE) pip install something "],["2-modeling-in-r-with-tidymodels.html", "2 Modeling in R With Tidymodels 2.1 Tidymodels Packages 2.2 Explore Data 2.3 Split Data 2.4 Prepare Data 2.5 Specify Model 2.6 Create Workflow 2.7 Train Model and Tune Parameters 2.8 Build model on all training data, test on validation set. 2.9 Build model on all training and validation data", " 2 Modeling in R With Tidymodels The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org/ Many modeling techniques in R require different syntax and different data structures. Tidymodels provides modeling workflow that standardizes syntax and data structures regardless of the model type. 2.1 Tidymodels Packages Like the tidyverse, tidymodels is a meta package consisting of the following packages: {rsample}: Creates different types of resamples and corresponding classes for analysis {recipes}: Uses dplyr-like pipeable sequences of feature engineering steps to get data ready for modeling {workflows}: Creates an object that can bundle together your pre-processing, modeling, and post-processing steps {parsnip}: Provides a tidy, unified interface to models than can by used to try a range of models without getting bogged down in the syntactical ninutae of the underslying packages. {tune}: Facilitates hyperparameter tuning for the tidymodels packages. {yardstick}: Estimates how well models are working using tidy data principles. {infer}: Performs statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework. 2.1.1 Tidymodels Road Map What we plan to do: Explore data Create model {rsample} Split data into test/train {recipies} Preprocess data {parsnip} Specify model {workflows} Create workflow {tune} / {dials} Train and tune parameters {parsnip} Finalize model {yardstick} Validate model Predict on new data 2.1.2 Modeling Goal We desire to create a model using the historical data and use that model to predict who may make the Hall-of-Fame in the eligible data. 2.2 Explore Data library(tidyverse) historical &lt;- read_csv(&quot;01_data/historical_baseball.csv&quot;) %&gt;% mutate(inducted = as.factor(inducted)) eligible &lt;- read_csv(&quot;01_data/eligible_baseball.csv&quot;) historical ## # A tibble: 3,235 x 15 ## player_id inducted g ab r h x2b x3b hr rbi sb cs bb so last_year ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 1 3298 12364 2174 3771 624 98 755 2297 240 73 1402 1383 1976 ## 2 aaronto01 0 437 944 102 216 42 6 13 94 9 8 86 145 1971 ## 3 aasedo01 0 448 5 0 0 0 0 0 0 0 0 0 3 1990 ## 4 abbated01 0 855 3044 355 772 99 43 11 324 142 0 289 16 1910 ## 5 abbotgl01 0 248 0 0 0 0 0 0 0 0 0 0 0 1984 ## 6 abbotji01 0 263 21 0 2 0 0 0 3 0 0 0 10 1999 ## 7 abernte02 0 681 181 12 25 3 0 0 9 0 0 6 74 1972 ## 8 ackerji01 0 467 54 2 9 1 0 0 1 0 0 2 32 1992 ## 9 adairje01 0 1165 4019 378 1022 163 19 57 366 29 29 208 499 1970 ## 10 adamsba01 0 482 1019 79 216 31 15 3 75 1 1 53 177 1926 ## # ... with 3,225 more rows The historical data contains career statistics of every baseball batter from 1880-2011 who no longer meet Hall-of-Fame eligability requirements or who have already made the hall-of-fame. Hall of Fame Qualifications - Play at least 10 years - Retired for at least 5 years - Players have only 10 years of eligability The eligible data contains everyone who is still eligible for the Hall-of-Fame You can see from the data below, the players who make the Hall-of-Fame tend to perform better in a few standard baseball statistics. historical %&gt;% select(-last_year) %&gt;% group_by(inducted) %&gt;% summarise(across(.cols = where(is.numeric), .fns = ~round(mean(.),0))) %&gt;% gt::gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #lsgowvjeoi .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lsgowvjeoi .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lsgowvjeoi .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lsgowvjeoi .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #lsgowvjeoi .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lsgowvjeoi .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lsgowvjeoi .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lsgowvjeoi .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lsgowvjeoi .gt_column_spanner_outer:first-child { padding-left: 0; } #lsgowvjeoi .gt_column_spanner_outer:last-child { padding-right: 0; } #lsgowvjeoi .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #lsgowvjeoi .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #lsgowvjeoi .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lsgowvjeoi .gt_from_md > :first-child { margin-top: 0; } #lsgowvjeoi .gt_from_md > :last-child { margin-bottom: 0; } #lsgowvjeoi .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lsgowvjeoi .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #lsgowvjeoi .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lsgowvjeoi .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #lsgowvjeoi .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lsgowvjeoi .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lsgowvjeoi .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lsgowvjeoi .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lsgowvjeoi .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lsgowvjeoi .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #lsgowvjeoi .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lsgowvjeoi .gt_sourcenote { font-size: 90%; padding: 4px; } #lsgowvjeoi .gt_left { text-align: left; } #lsgowvjeoi .gt_center { text-align: center; } #lsgowvjeoi .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lsgowvjeoi .gt_font_normal { font-weight: normal; } #lsgowvjeoi .gt_font_bold { font-weight: bold; } #lsgowvjeoi .gt_font_italic { font-style: italic; } #lsgowvjeoi .gt_super { font-size: 65%; } #lsgowvjeoi .gt_footnote_marks { font-style: italic; font-size: 65%; } inducted g ab r h x2b x3b hr rbi sb cs bb so 0 792 2330 305 612 99 23 40 273 48 16 215 269 1 1649 5794 934 1704 288 75 146 852 161 36 627 551 The plot of the data supports this as well. historical %&gt;% pivot_longer(g:so) %&gt;% ggplot(aes(x = inducted, y = value)) + geom_boxplot() + facet_wrap(~name, scales = &quot;free&quot;) + labs(y = &quot;&quot;,x = &quot;Hall-of-Fame Indicator&quot;) 2.3 Split Data To begin the analysis, we will load the {tidymodels} library. library(tidymodels) We will split the data into a training (2/3s of the data) and testing set (1/3) of the data. We set the seed so the analysis is reproducible. The output of this function is an rsplit object. An rsplit object that can be used with the training and testing functions to extract the data in each split. set.seed(42) data_split &lt;- initial_split(historical, prop = 2/3, strata = inducted) data_split ## &lt;Analysis/Assess/Total&gt; ## &lt;2156/1079/3235&gt; We can extract the data from the rsplit object. train_data &lt;- training(data_split) test_data &lt;- testing(data_split) train_data ## # A tibble: 2,156 x 15 ## player_id inducted g ab r h x2b x3b hr rbi sb cs bb so last_year ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 1 3298 12364 2174 3771 624 98 755 2297 240 73 1402 1383 1976 ## 2 aaronto01 0 437 944 102 216 42 6 13 94 9 8 86 145 1971 ## 3 aasedo01 0 448 5 0 0 0 0 0 0 0 0 0 3 1990 ## 4 abbated01 0 855 3044 355 772 99 43 11 324 142 0 289 16 1910 ## 5 abbotji01 0 263 21 0 2 0 0 0 3 0 0 0 10 1999 ## 6 abernte02 0 681 181 12 25 3 0 0 9 0 0 6 74 1972 ## 7 ackerji01 0 467 54 2 9 1 0 0 1 0 0 2 32 1992 ## 8 adamsba01 0 482 1019 79 216 31 15 3 75 1 1 53 177 1926 ## 9 adamsbo03 0 1281 4019 591 1082 188 49 37 303 67 30 414 447 1959 ## 10 adamssp01 0 1424 5557 844 1588 249 48 9 394 154 50 453 223 1934 ## # ... with 2,146 more rows From the training data, we further split the data into a training (2/3 of the training data) and a validation set (1/3 of the training data) for parameter tuning and model assessment. set.seed(42) validation_set &lt;- validation_split(data = train_data, prop = 2/3, strata = inducted) validation_set ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1437/719]&gt; validation 2.4 Prepare Data What preprocessing steps do you want to do to your date every time you model? We need to specify the following things: - Specify the modeling formula - Specify the roles of each of the factors - Do all preprocessing steps In the {tidymodels} construct, we do this by creating a recipe. baseball_recipie &lt;- recipe(inducted ~ ., data = train_data) %&gt;% update_role(player_id, new_role = &quot;ID&quot;) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_nzv(all_numeric()) %&gt;% step_rm(&quot;last_year&quot;) baseball_recipie ## Data Recipe ## ## Inputs: ## ## role #variables ## ID 1 ## outcome 1 ## predictor 13 ## ## Operations: ## ## Centering for all_numeric() ## Scaling for all_numeric() ## Sparse, unbalanced variable filter on all_numeric() ## Delete terms &quot;last_year&quot; 2.5 Specify Model Now that weve prepared our data, we need to specify the model we wish to execute. Here we identify the model type, specify parameters which need tuning, and then set the engine (package) we wish to do the work. lr_mod &lt;- logistic_reg(mode = &quot;classification&quot;, penalty = tune(), mixture = 1) %&gt;% set_engine(engine = &quot;glmnet&quot;) lr_mod ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet 2.6 Create Workflow Now that weve prepared the data and specified the model, we put it all together in a workflow. In a workflow, we add the specified model and the preprocessing recipe. baseball_workflow &lt;- workflow() %&gt;% add_model(lr_mod) %&gt;% add_recipe(baseball_recipie) baseball_workflow ## == Workflow ========================================================================================================================================= ## Preprocessor: Recipe ## Model: logistic_reg() ## ## -- Preprocessor ------------------------------------------------------------------------------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model -------------------------------------------------------------------------------------------------------------------------------------------- ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet 2.7 Train Model and Tune Parameters This step not only executes the model building proceedure, but it contains the structure to execute the model over a grid of parameters to tune the model and find the best parameter set. First we specify the parameters over which we desire to tune. lr_reg_grid &lt;- tibble(penalty = 10^seq(-4, -1, length.out = 30)) Next, we use tune_grid() to execute the model one time for each parameter set. In this instance, this is 30 times. This function has several arguments grid: The tibble we created that contains the parameters we have specified. control: Controls the apects of the grid search process. metrics: Specifies the model quality metrics we wish to save for each model in cross validation. We also specify that we wish to save the performance metrics for each of the 30 iterations. set.seed(42) lr_validation &lt;- baseball_workflow %&gt;% tune_grid(validation_set, grid = lr_reg_grid, control = control_grid(save_pred = TRUE, verbose = TRUE, allow_par = FALSE), metrics = metric_set(roc_auc, accuracy)) lr_validation ## # Tuning results ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [1437/719]&gt; validation &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [21,570 x 7]&gt; Here, we extract out the best 25 models based on accuracy and plot them vs the pentalty from the tuning parameter grid. lr_validation %&gt;% show_best(&quot;accuracy&quot;, n = 25) %&gt;% arrange(penalty) %&gt;% as.data.frame() %&gt;% ggplot(aes(x = penalty, y = mean)) + geom_point() + geom_line() + scale_x_log10() We desire the pentalty that gives us the simplest model, with the best accuracy. Because of that, we select the 7th smallest penalty. lr_best &lt;- lr_validation %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% slice(7) We show the ROC curve for the selected model. lr_validation %&gt;% collect_predictions(parameters = lr_best) %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.8 Build model on all training data, test on validation set. Now that weve found the best parameter set, we need to apply this model to the entire training dataset. We made one tweek to our model specification. We specify the specific penalry from our best model from cross validation. last_lr_mod &lt;- logistic_reg(mode = &quot;classification&quot;, penalty = lr_best$penalty, mixture = 1) %&gt;% set_engine(engine = &quot;glmnet&quot;) last_lr_mod ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = lr_best$penalty ## mixture = 1 ## ## Computational engine: glmnet We update our workflow to have the best parameter set with the function finalize_workflow(). last_lr_workflow &lt;- baseball_workflow %&gt;% finalize_workflow(lr_best) last_lr_workflow ## == Workflow ========================================================================================================================================= ## Preprocessor: Recipe ## Model: logistic_reg() ## ## -- Preprocessor ------------------------------------------------------------------------------------------------------------------------------------- ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model -------------------------------------------------------------------------------------------------------------------------------------------- ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0.00041753189365604 ## mixture = 1 ## ## Computational engine: glmnet We fit the model on the entire training set. last_lr_fit &lt;- last_lr_workflow %&gt;% last_fit(data_split) We can see the performance of the model below. last_lr_fit %&gt;% collect_metrics() ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.947 Preprocessor1_Model1 ## 2 roc_auc binary 0.778 Preprocessor1_Model1 And we can take a view look at the ROC of our final model. last_lr_fit %&gt;% collect_predictions() %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.9 Build model on all training and validation data Now, we can use the fit() function to build the model on the entire historical data. last_lr_workflow %&gt;% fit(data = historical) %&gt;% pull_workflow_fit() %&gt;% tidy() ## # A tibble: 13 x 3 ## term estimate penalty ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -3.21 0.000418 ## 2 g 1.68 0.000418 ## 3 ab -3.11 0.000418 ## 4 r 1.43 0.000418 ## 5 h 0.988 0.000418 ## 6 x2b 0 0.000418 ## 7 x3b 0.118 0.000418 ## 8 hr 0 0.000418 ## 9 rbi 0.477 0.000418 ## 10 sb 0 0.000418 ## 11 cs -0.190 0.000418 ## 12 bb -0.250 0.000418 ## 13 so -0.0290 0.000418 Now that we have the model, we can make predictions on the eligible data. How did we do? last_lr_workflow %&gt;% fit(data = historical) %&gt;% predict(eligible, type = &quot;prob&quot;) %&gt;% bind_cols(eligible) %&gt;% arrange(-.pred_1) %&gt;% filter(.pred_1 &gt;.4) %&gt;% mutate(across(contains(&quot;pred&quot;), ~round(.,3))) %&gt;% # print(n = Inf) %&gt;% DT::datatable() "],["3-modeling-in-python-with-scikit-learn.html", "3 Modeling in python with scikit-learn 3.1 Scikit-learn Overview 3.2 Explore Data 3.3 Split Data 3.4 Define a Pipeline 3.5 Fit the Model (Using the Pipeline) 3.6 Score and Evaluate the Model", " 3 Modeling in python with scikit-learn 3.1 Scikit-learn Overview Like {tidymodels}, scikit-learn offers a suite of tools for predictive modeling and machine learning: it will help you split data, preprocess model inputs, fit models, and compare and assess them. Unlike {tidymodels}, scikit-learn is a single monolithic package with functions for the entire modeling pipeline. Users will likely still want to use pandas to ingest and prepare data, and may want to use other libraries to supplement scikit-learns data visualiation capabilities, but scikit-learn will do most of the work by itself - and often with less and simpler code than tidymodels, at least for standard machine learning workflows. 3.1.1 Tidymodels Road Map What we plan to do: Read in and explore data (pandas and R) Create model (scikit-learn) split data define pipeline with preprocessors and model with cross-validation for parameter tuning fit model Predict on new data and assess model (scikit-learn) 3.1.2 Modeling Goal We plan to create a model using the historical data and use that model to predict who is most likely to make it into the Hall of Fame in the eligible data. 3.2 Explore Data Well load the pandas library to import and set up the data. import pandas as pd import numpy as np Here, we use pandas read_csv() to import the data, and then we print the first few rows of the historical dataframe to the console. historical = pd.read_csv(&#39;01_data/historical_baseball.csv&#39;) eligible = pd.read_csv(&#39;01_data/eligible_baseball.csv&#39;) historical ## player_id inducted g ab r ... sb cs bb so last_year ## 0 aaronha01 1 3298 12364 2174 ... 240 73 1402 1383 1976 ## 1 aaronto01 0 437 944 102 ... 9 8 86 145 1971 ## 2 aasedo01 0 448 5 0 ... 0 0 0 3 1990 ## 3 abbated01 0 855 3044 355 ... 142 0 289 16 1910 ## 4 abbotgl01 0 248 0 0 ... 0 0 0 0 1984 ## ... ... ... ... ... ... ... ... .. ... ... ... ## 3230 ziskri01 0 1453 5144 681 ... 8 15 533 910 1983 ## 3231 zitzmbi01 0 406 1004 197 ... 42 11 83 85 1929 ## 3232 zoskyed01 0 44 50 4 ... 0 0 1 13 2000 ## 3233 zuberbi01 0 224 229 10 ... 0 0 10 66 1947 ## 3234 zuvelpa01 0 209 491 41 ... 2 0 34 50 1991 ## ## [3235 rows x 15 columns] The historical data contains career statistics of every baseball batter from 1880-2011 who no longer meet Hall-of-Fame eligibility requirements or who have already made the hall-of-fame. Hall of Fame Qualifications - Play at least 10 years - Retired for at least 5 years - Players have only 10 years of eligability The eligible data contains everyone who is still eligible for the Hall-of-Fame You can see from the data below, the players who make the Hall-of-Fame tend to perform better in a few standard baseball statistics. This pandas code does the same thing as the R code in the previous chapter - it groups the historical data by whether or not the player was inducted into the Hall of Fame, and then takes the mean of each column in each group. hist_means_inducted_groups = historical.drop(&#39;last_year&#39;, axis = 1) \\ .groupby(&#39;inducted&#39;) \\ .mean() \\ .round() We bring the data back into R, using RStudios very simple Python-R interface, and use the same R code as in Chapter 1 to print a table and produce boxplots of the means. py$hist_means_inducted_groups %&gt;% rownames_to_column(var = &quot;inducted&quot;) %&gt;% gt::gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ujoigdioba .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ujoigdioba .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ujoigdioba .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ujoigdioba .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #ujoigdioba .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ujoigdioba .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ujoigdioba .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ujoigdioba .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ujoigdioba .gt_column_spanner_outer:first-child { padding-left: 0; } #ujoigdioba .gt_column_spanner_outer:last-child { padding-right: 0; } #ujoigdioba .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #ujoigdioba .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ujoigdioba .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ujoigdioba .gt_from_md > :first-child { margin-top: 0; } #ujoigdioba .gt_from_md > :last-child { margin-bottom: 0; } #ujoigdioba .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ujoigdioba .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ujoigdioba .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ujoigdioba .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ujoigdioba .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ujoigdioba .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ujoigdioba .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ujoigdioba .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ujoigdioba .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ujoigdioba .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #ujoigdioba .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ujoigdioba .gt_sourcenote { font-size: 90%; padding: 4px; } #ujoigdioba .gt_left { text-align: left; } #ujoigdioba .gt_center { text-align: center; } #ujoigdioba .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ujoigdioba .gt_font_normal { font-weight: normal; } #ujoigdioba .gt_font_bold { font-weight: bold; } #ujoigdioba .gt_font_italic { font-style: italic; } #ujoigdioba .gt_super { font-size: 65%; } #ujoigdioba .gt_footnote_marks { font-style: italic; font-size: 65%; } inducted g ab r h x2b x3b hr rbi sb cs bb so 0 792 2330 305 612 99 23 40 273 48 16 215 269 1 1649 5794 934 1704 288 75 146 852 161 36 627 551 py$historical %&gt;% group_by(inducted) %&gt;% pivot_longer(g:so) %&gt;% ggplot(aes(x = inducted, y = value)) + geom_boxplot() + facet_wrap(~name, scales = &quot;free&quot;) + labs(y = &quot;&quot;,x = &quot;Hall-of-Fame Indicator&quot;) 3.3 Split Data As we did in R, we will split the data into a training set (2/3s of the data) and testing set (1/3) of the data. We set the seed so the analysis is reproducible - here, we do this using the random_state parameter in train_test_split(). Instead of an rsplit object that contains resampling metadata, train_test_split() returns four objects: X (predictor) pandas DataFrame objects for the training and test sets, and y (target) pandas Series objects for the training and test sets. Note that before splitting the data, we set the index of the dataframe to be player_id. This carries through to the outputs of train_test_split(), which all have player_id as a common index (and not as a predictor or target variable.) In a way, this serves a similar purpose to the update_role(player_id, new_role = \"ID\") line that we added to the recipe in R. from sklearn.model_selection import train_test_split historical_pidindex = historical.set_index(&#39;player_id&#39;) X = historical_pidindex.drop([&#39;inducted&#39;, &#39;last_year&#39;], axis = 1) y = historical_pidindex.inducted X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size = 1/3) 3.4 Define a Pipeline Scikit-learns pipelines serve the combined purpose of workflows and recipes in {tidymodels}. They allow you to define a set of preprocessing and modeling steps that you can then apply to any dataset. They are defined by the function make_pipeline(), with the steps, in order, as arguments. The first two steps in our pipeline will take care of preprocessing. In the previous chapter, we centered and scaled our data; here, well use StandardScaler(), which accomplishes both of those steps. Well also apply VarianceThreshold(); in its default form, this only removes zero-variance predictors, but the user can set a custom variance threshold. None of our predictors have low variance, so this feature selection mechanism does nothing anyway. The third step in our pipeline is our model. Here, weve chosen LogisticRegressionCV(). The first three parameters should produce a model very similar to the one in the previous chapter: - penalty = \"elasticnet\" lets us use a hybrid L1 and L2 penalty, or a mix between Lasso and Ridge regression, much like engine = glmnet in R; - solver = \"saga\" chooses a solver that is compatible with our other options; - l1_ratios = [1.0] is the equivalent of mixture = 1 in R - it gives us a pure Lasso regression; - max_iter = 1000 allows the solver to attempt up to 1000 iterations as it searches for a solution. The default of 100 was insufficient for this data. We also have one parameter related to the cross-validation (CV) part of the model specification: cv = 10. This means that the data will be split into ten folds, and the model will be fit ten times, with one fold being held out as a validation set each time. This process will allow the model to tune the size of penalty, which we have not specified explicitly. NOTE: MAKE SURE THAT IM CORRECTLY PORTRAYING HOW LOGISTICREGRESSIONCV USES CROSS VALIDATION from sklearn import preprocessing from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.feature_selection import VarianceThreshold # l1_ratio pipe_scale_lr_lasso = make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegressionCV(penalty = &quot;elasticnet&quot;, solver = &quot;saga&quot;, l1_ratios = [0.5, 1.0], cv = 10, max_iter = 1000)) It is also possible to use a parameter tuning method more like the onein the previous chapter, using gridsearchCV and a predefined grid of search values The scikit-learn user guide has a very detailed section on this method, available at: https://scikit-learn.org/stable/modules/grid_search.html 3.5 Fit the Model (Using the Pipeline) With our pipeline defined, fitting the model on the training data is very easy: we simply call the fit() method on the pipeline, with our X_train and y_train data as the inputs. pipe_scale_lr_lasso.fit(X_train, y_train) # apply scaling on training data ## Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), ## (&#39;variancethreshold&#39;, VarianceThreshold()), ## (&#39;logisticregressioncv&#39;, ## LogisticRegressionCV(cv=10, l1_ratios=[0.5, 1.0], ## max_iter=1000, penalty=&#39;elasticnet&#39;, ## solver=&#39;saga&#39;))]) 3.6 Score and Evaluate the Model The most basic way to assess the performance of a fitted scikit-learn model is the score() function, with the test set as inputs. This uses the fitted model to predict on the test set and returns the proportion of correct predictions. pipe_scale_lr_lasso.score(X_test, y_test) ## 0.9397590361445783 pipe_scale_lr = make_pipeline(StandardScaler(), VarianceThreshold(), LogisticRegression()) pipe_scale_lr.fit(X_train, y_train) # apply scaling on training data ## Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), ## (&#39;variancethreshold&#39;, VarianceThreshold()), ## (&#39;logisticregression&#39;, LogisticRegression())]) pipe_scale_lr.score(X_test, y_test) ## 0.9397590361445783 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
