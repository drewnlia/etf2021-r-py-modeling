[["index.html", "Git and Differential Equations in R 1 Class Introduction 1.1 Topics &amp; Class Structure 1.2 Software Prerequisites 1.3 Human Prerequisites 1.4 Distance Learning Challenges 1.5 Endstate 1.6 Instructors Introduction 1.7 Lets Get Started", " Git and Differential Equations in R MAJ Dusty Turner and Robert Ward 7 DEC 2020 1 Class Introduction Disclaimer: The appearance of U.S. Department of Defense (DoD) visual information does not imply or constitute DoD endorsement. The views expressed in this presentation are those only of the author and do not represent the official position of the U.S. Army, DoD, or the federal government. 1.1 Topics &amp; Class Structure Git/GitHub setup and repository download (~45 minutes; interactive) Ten-minute break Using R for modeling with differential equations (~30 minutes; interactive) Collaboration using Git (as time allows; instructor demonstration) 1.2 Software Prerequisites R 3.6.x or newer RStudio 1.2.x or newer Git Git Bash (included with Git for Windows) or another command-line application of your choice capable of using Git A github account github website 1.3 Human Prerequisites We assume you have: Working knowledge of R and RStudio; Some experience with contemporary tidy coding concepts; A need to work collaboratively on projects; A healthy appreciation of dad jokes. Try to follow along, at least through the differential equation section. Its okay to struggle to keep up with certain parts of this - well do our best to help you, but we also wont be offended if you decide to leave. 1.4 Distance Learning Challenges Dont be afraid to ask questions - both verbally and in chat. If you miss something we said, its likely others have too - youll be helping them by speaking up. Its difficult the know the speed of the class, so please communicate! 1.5 Endstate You have git configured on your computer; You have linked git to github; You have a repo cloned to your computer; You can push, pull, and merge git conflicts in the repository; Youve learned the basics of using differential equations in R. 1.6 Instructors Introduction 1.6.1 MAJ Dusty Turner Army Combat Engineer Platoon Leader / XO / Company Commander Geospatial / Sapper / Route Clearance Hawaii / White Sands Missile Range / Iraq / Afghanistan Education West Point 07 Operations Research, BS Missouri University of Science and Technology 12 Engineering Management, MS THE Ohio State 16 Integrated Systems Engineering, MS Applied Statistics, Graduate Minor Data Science R User Since 14 Catch me on Twitter @dtdusty http://dustysturner.com/ 1.6.2 Robert Ward Education University of Chicago, 13 Political Science &amp; English, BA Columbia University School of International and Public Affairs, 18 Master of International Affairs, Specialization in Advanced Policy and Economic Analysis Data Science R user since 2011; also know some python and forgot some Stata Worked for Government Accountability Office Applied Research &amp; Methods Operations Research Systems Analyst at CAA and Army Leader Dashboard/Vantage PM team 1.7 Lets Get Started 1.7.1 Prerequisite Packages install.packages(c(&quot;tidyverse&quot;, &quot;deSolve&quot;), dependencies = TRUE) "],["2-modeling-in-r-with-tidymodels.html", "2 Modeling in R With Tidymodels 2.1 Tidymodels Packages 2.2 Explore Data 2.3 Split Data 2.4 Prepare Data 2.5 Specify Model 2.6 Create Workflow 2.7 Train Model and Tune Parameters 2.8 Build model on all training data, test on validation set. 2.9 Build model on all training and validation data", " 2 Modeling in R With Tidymodels The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org/ Many modeling techniques in R require different syntax and different data structures. Tidymodels provides modeling workflow that standardizes syntax and data structures regardless of the model type. 2.1 Tidymodels Packages Like the tidyverse, tidymodels is a meta package consisting of the following packages: {rsample}: Creates different types of resamples and corresponding classes for analysis {recipes}: Uses dplyr-like pipeable sequences of feature engineering steps to get data ready for modeling {workflows}: Creates an object that can bundle together your pre-processing, modeling, and post-processing steps {parsnip}: Provides a tidy, unified interface to models than can by used to try a range of models without getting bogged down in the syntactical ninutae of the underslying packages. {tune}: Facilitates hyperparameter tuning for the tidymodels packages. {yardstick}: Estimates how well models are working using tidy data principles. {infer}: Performs statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework. 2.1.1 Tidymodels Road Map What we plan to do: Explore data Create model {rsample} Split data into test/train {recipies} Preprocess data {parsnip} Specify model {workflows} Create workflow {tune} / {dials} Train and tune parameters {parsnip} Finalize model {yardstick} Validate model Predict on new data 2.1.2 Modeling Goal We desire to create a model using the historical data and use that model to predict who may make the Hall-of-Fame in the eligible data. 2.2 Explore Data library(tidyverse) historical &lt;- read_csv(&quot;01_data/historical_baseball.csv&quot;, col_types = cols(inducted = col_factor())) eligible &lt;- read_csv(&quot;01_data/eligible_baseball.csv&quot;) historical ## # A tibble: 3,235 x 15 ## player_id inducted g ab r h x2b x3b hr rbi sb cs bb so last_year ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 1 3298 12364 2174 3771 624 98 755 2297 240 73 1402 1383 1976 ## 2 aaronto01 0 437 944 102 216 42 6 13 94 9 8 86 145 1971 ## 3 aasedo01 0 448 5 0 0 0 0 0 0 0 0 0 3 1990 ## 4 abbated01 0 855 3044 355 772 99 43 11 324 142 0 289 16 1910 ## 5 abbotgl01 0 248 0 0 0 0 0 0 0 0 0 0 0 1984 ## 6 abbotji01 0 263 21 0 2 0 0 0 3 0 0 0 10 1999 ## 7 abernte02 0 681 181 12 25 3 0 0 9 0 0 6 74 1972 ## 8 ackerji01 0 467 54 2 9 1 0 0 1 0 0 2 32 1992 ## 9 adairje01 0 1165 4019 378 1022 163 19 57 366 29 29 208 499 1970 ## 10 adamsba01 0 482 1019 79 216 31 15 3 75 1 1 53 177 1926 ## # ... with 3,225 more rows The historical data contains career statistics of every baseball batter from 1880-2011 who no longer meet Hall-of-Fame eligability requirements or who have already made the hall-of-fame. Hall of Fame Qualifications - Play at least 10 years - Retired for at least 5 years - Players have only 10 years of eligability The eligible data contains everyone who is still eligible for the Hall-of-Fame You can see from the data below, the players who make the Hall-of-Fame tend to perform better in a few standard baseball statistics. historical %&gt;% select(-last_year) %&gt;% group_by(inducted) %&gt;% summarise(across(.cols = where(is.numeric), .fns = ~round(mean(.),0))) %&gt;% gt::gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #zrpmkzikre .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #zrpmkzikre .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zrpmkzikre .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #zrpmkzikre .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #zrpmkzikre .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zrpmkzikre .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #zrpmkzikre .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #zrpmkzikre .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #zrpmkzikre .gt_column_spanner_outer:first-child { padding-left: 0; } #zrpmkzikre .gt_column_spanner_outer:last-child { padding-right: 0; } #zrpmkzikre .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #zrpmkzikre .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #zrpmkzikre .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #zrpmkzikre .gt_from_md > :first-child { margin-top: 0; } #zrpmkzikre .gt_from_md > :last-child { margin-bottom: 0; } #zrpmkzikre .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #zrpmkzikre .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #zrpmkzikre .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zrpmkzikre .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #zrpmkzikre .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #zrpmkzikre .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #zrpmkzikre .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #zrpmkzikre .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #zrpmkzikre .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zrpmkzikre .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #zrpmkzikre .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #zrpmkzikre .gt_sourcenote { font-size: 90%; padding: 4px; } #zrpmkzikre .gt_left { text-align: left; } #zrpmkzikre .gt_center { text-align: center; } #zrpmkzikre .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #zrpmkzikre .gt_font_normal { font-weight: normal; } #zrpmkzikre .gt_font_bold { font-weight: bold; } #zrpmkzikre .gt_font_italic { font-style: italic; } #zrpmkzikre .gt_super { font-size: 65%; } #zrpmkzikre .gt_footnote_marks { font-style: italic; font-size: 65%; } inducted g ab r h x2b x3b hr rbi sb cs bb so 1 1649 5794 934 1704 288 75 146 852 161 36 627 551 0 792 2330 305 612 99 23 40 273 48 16 215 269 The plot of the data supports this as well. historical %&gt;% pivot_longer(g:so) %&gt;% ggplot(aes(x = inducted, y = value)) + geom_boxplot() + facet_wrap(~name, scales = &quot;free&quot;) + labs(y = &quot;&quot;,x = &quot;Hall-of-Fame Indicator&quot;) 2.3 Split Data To begin the analysis, we will load the {tidymodels} library. library(tidymodels) We will split the data into a training (2/3s of the data) and testing set (1/3) of the data. We set the seed so the analysis is reproducible. The output of this function is an rsplit object. An rsplit object that can be used with the training and testing functions to extract the data in each split. set.seed(42) data_split &lt;- initial_split(historical, prop = 2/3, strata = inducted) data_split ## &lt;Analysis/Assess/Total&gt; ## &lt;2157/1078/3235&gt; We can extract the data from the rsplit object. train_data &lt;- training(data_split) test_data &lt;- testing(data_split) train_data ## # A tibble: 2,157 x 15 ## player_id inducted g ab r h x2b x3b hr rbi sb cs bb so last_year ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 1 3298 12364 2174 3771 624 98 755 2297 240 73 1402 1383 1976 ## 2 abbotgl01 0 248 0 0 0 0 0 0 0 0 0 0 0 1984 ## 3 abbotji01 0 263 21 0 2 0 0 0 3 0 0 0 10 1999 ## 4 abernte02 0 681 181 12 25 3 0 0 9 0 0 6 74 1972 ## 5 adairje01 0 1165 4019 378 1022 163 19 57 366 29 29 208 499 1970 ## 6 adamsbe01 0 267 678 37 137 17 4 2 45 9 0 23 79 1919 ## 7 ageeto01 0 1129 3912 558 999 170 27 130 433 167 81 342 918 1973 ## 8 aguilri01 0 737 139 12 28 3 0 3 11 0 0 6 37 2000 ## 9 ainsmed01 0 1078 3048 299 707 108 54 22 317 86 16 263 315 1924 ## 10 aldremi01 0 930 2147 277 565 104 9 41 271 19 18 314 381 1996 ## # ... with 2,147 more rows From the training data, we further split the data into a training (2/3 of the training data) and a validation set (1/3 of the training data) for parameter tuning and model assessment. set.seed(42) validation_set &lt;- validation_split(data = train_data, prop = 2/3, strata = inducted) validation_set ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1438/719]&gt; validation 2.4 Prepare Data What preprocessing steps do you want to do to your date every time you model? We need to specify the following things: - Specify the modeling formula - Specify the roles of each of the factors - Do all preprocessing steps In the {tidymodels} construct, we do this by creating a recipe. baseball_recipie &lt;- recipe(inducted ~ ., data = train_data) %&gt;% update_role(player_id, new_role = &quot;ID&quot;) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_nzv(all_numeric()) %&gt;% step_rm(&quot;last_year&quot;) baseball_recipie ## Data Recipe ## ## Inputs: ## ## role #variables ## ID 1 ## outcome 1 ## predictor 13 ## ## Operations: ## ## Centering for all_numeric() ## Scaling for all_numeric() ## Sparse, unbalanced variable filter on all_numeric() ## Delete terms &quot;last_year&quot; 2.5 Specify Model Now that weve prepared our data, we need to specify the model we wish to execute. Here we identify the model type, specify parameters which need tuning, and then set the engine (package) we wish to do the work. lr_mod &lt;- logistic_reg(mode = &quot;classification&quot;, penalty = tune(), mixture = 1) %&gt;% set_engine(engine = &quot;glmnet&quot;) lr_mod ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet 2.6 Create Workflow Now that weve prepared the data and specified the model, we put it all together in a workflow. In a workflow, we add the specified model and the preprocessing recipe. baseball_workflow &lt;- workflow() %&gt;% add_model(lr_mod) %&gt;% add_recipe(baseball_recipie) baseball_workflow ## == Workflow ============================================================================================================================================== ## Preprocessor: Recipe ## Model: logistic_reg() ## ## -- Preprocessor ------------------------------------------------------------------------------------------------------------------------------------------ ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model ------------------------------------------------------------------------------------------------------------------------------------------------- ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = 1 ## ## Computational engine: glmnet 2.7 Train Model and Tune Parameters This step not only executes the model building proceedure, but it contains the structure to execute the model over a grid of parameters to tune the model and find the best parameter set. First we specify the parameters over which we desire to tune. lr_reg_grid &lt;- tibble(penalty = 10^seq(-4, -1, length.out = 30)) Next, we use tune_grid() to execute the model one time for each parameter set. In this instance, this is 30 times. This function has several arguments grid: The tibble we created that contains the parameters we have specified. control: Controls the apects of the grid search process. metrics: Specifies the model quality metrics we wish to save for each model in cross validation. We also specify that we wish to save the performance metrics for each of the 30 iterations. set.seed(42) lr_validation &lt;- baseball_workflow %&gt;% tune_grid(validation_set, grid = lr_reg_grid, control = control_grid(save_pred = TRUE, verbose = TRUE, allow_par = FALSE), metrics = metric_set(roc_auc, accuracy)) lr_validation ## # Tuning results ## # Validation Set Split (0.67/0.33) using stratification ## # A tibble: 1 x 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [1438/719]&gt; validation &lt;tibble [60 x 5]&gt; &lt;tibble [0 x 1]&gt; &lt;tibble [21,570 x 7]&gt; Here, we extract out the best 25 models based on accuracy and plot them vs the pentalty from the tuning parameter grid. lr_validation %&gt;% show_best(&quot;accuracy&quot;, n = 25) %&gt;% arrange(penalty) %&gt;% as.data.frame() %&gt;% ggplot(aes(x = penalty, y = mean)) + geom_point() + geom_line() + scale_x_log10() We desire the pentalty that gives us the simplest model, with the best accuracy. Because of that, we select the 7th smallest penalty. lr_best &lt;- lr_validation %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% slice(7) We show the ROC curve for the selected model. lr_validation %&gt;% collect_predictions(parameters = lr_best) %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.8 Build model on all training data, test on validation set. Now that weve found the best parameter set, we need to apply this model to the entire training dataset. We made one tweek to our model specification. We specify the specific penalry from our best model from cross validation. last_lr_mod &lt;- logistic_reg(mode = &quot;classification&quot;, penalty = lr_best$penalty, mixture = 1) %&gt;% set_engine(engine = &quot;glmnet&quot;) last_lr_mod ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = lr_best$penalty ## mixture = 1 ## ## Computational engine: glmnet We update our workflow to have the best parameter set with the function finalize_workflow(). last_lr_workflow &lt;- baseball_workflow %&gt;% finalize_workflow(lr_best) last_lr_workflow ## == Workflow ============================================================================================================================================== ## Preprocessor: Recipe ## Model: logistic_reg() ## ## -- Preprocessor ------------------------------------------------------------------------------------------------------------------------------------------ ## 4 Recipe Steps ## ## * step_center() ## * step_scale() ## * step_nzv() ## * step_rm() ## ## -- Model ------------------------------------------------------------------------------------------------------------------------------------------------- ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0.00041753189365604 ## mixture = 1 ## ## Computational engine: glmnet We fit the model on the entire training set. last_lr_fit &lt;- last_lr_workflow %&gt;% last_fit(data_split) We can see the performance of the model below. last_lr_fit %&gt;% collect_metrics() ## # A tibble: 2 x 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.943 Preprocessor1_Model1 ## 2 roc_auc binary 0.807 Preprocessor1_Model1 And we can take a view look at the ROC of our final model. last_lr_fit %&gt;% collect_predictions() %&gt;% roc_curve(inducted, .pred_1) %&gt;% autoplot() 2.9 Build model on all training and validation data Now, we can use the fit() function to build the model on the enitre historical data. last_lr_workflow %&gt;% fit(data = historical) %&gt;% pull_workflow_fit() %&gt;% tidy() ## # A tibble: 13 x 3 ## term estimate penalty ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.21 0.000418 ## 2 g -1.68 0.000418 ## 3 ab 3.11 0.000418 ## 4 r -1.43 0.000418 ## 5 h -0.988 0.000418 ## 6 x2b 0 0.000418 ## 7 x3b -0.118 0.000418 ## 8 hr 0 0.000418 ## 9 rbi -0.477 0.000418 ## 10 sb 0 0.000418 ## 11 cs 0.190 0.000418 ## 12 bb 0.250 0.000418 ## 13 so 0.0290 0.000418 Now that we have the model, we can make predictions on the eligible data. How did we do? last_lr_workflow %&gt;% fit(data = historical) %&gt;% predict(eligible, type = &quot;prob&quot;) %&gt;% bind_cols(eligible) %&gt;% arrange(-.pred_1) %&gt;% filter(.pred_1 &gt;.4) %&gt;% print(n = Inf) ## # A tibble: 31 x 16 ## .pred_1 .pred_0 player_id g ab r h x2b x3b hr rbi sb cs bb so last_year ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.946 0.0535 bondsba01 2986 9847 2227 2935 601 77 762 1996 514 141 2558 1539 2007 ## 2 0.906 0.0941 rodrial01 2784 10566 2021 3115 548 31 696 2086 329 76 1338 2287 2016 ## 3 0.809 0.191 pujolal01 2426 9138 1670 2825 602 16 591 1817 107 41 1214 1053 2016 ## 4 0.792 0.208 ramirma02 2302 8244 1544 2574 547 20 555 1831 38 33 1329 1813 2011 ## 5 0.747 0.253 thomeji01 2543 8422 1583 2328 451 26 612 1699 19 20 1747 2548 2012 ## 6 0.745 0.255 palmera01 2831 10472 1663 3020 585 38 569 1835 97 40 1353 1348 2005 ## 7 0.734 0.266 jonesch06 2499 8984 1619 2726 549 38 468 1623 150 46 1512 1409 2012 ## 8 0.675 0.325 jeterde01 2747 11195 1923 3465 544 66 260 1311 358 97 1082 1840 2014 ## 9 0.661 0.339 sheffga01 2576 9217 1636 2689 467 27 509 1676 253 104 1475 1171 2009 ## 10 0.649 0.351 ortizda01 2408 8640 1419 2472 632 19 541 1768 17 9 1319 1750 2016 ## 11 0.628 0.372 heltoto01 2247 7962 1401 2519 592 37 369 1406 37 29 1335 1175 2013 ## 12 0.623 0.377 walkela01 1988 6907 1355 2160 471 62 383 1311 230 76 913 1231 2005 ## 13 0.596 0.404 cabremi01 2096 7853 1321 2519 523 17 446 1553 38 20 1011 1516 2016 ## 14 0.592 0.408 beltrca01 2457 9301 1522 2617 536 78 421 1536 312 49 1051 1693 2016 ## 15 0.535 0.465 beltrad01 2720 10295 1428 2942 591 36 445 1571 119 42 775 1584 2016 ## 16 0.528 0.472 guerrvl01 2147 8155 1328 2590 477 46 449 1496 181 94 737 985 2011 ## 17 0.525 0.475 giambja01 2260 7267 1227 2010 405 9 440 1441 20 12 1366 1572 2014 ## 18 0.509 0.491 gonzalu01 2591 9157 1412 2591 596 68 354 1439 128 87 1155 1218 2008 ## 19 0.508 0.492 kentje01 2298 8498 1320 2461 560 47 377 1518 94 60 801 1522 2008 ## 20 0.499 0.501 sosasa01 2354 8813 1475 2408 379 45 609 1667 234 107 929 2306 2007 ## 21 0.496 0.504 damonjo01 2490 9736 1668 2769 522 109 235 1139 408 103 1003 1257 2012 ## 22 0.483 0.517 mcgrifr01 2460 8757 1349 2490 441 24 493 1550 72 38 1305 1882 2004 ## 23 0.478 0.522 delgaca01 2035 7283 1241 2038 483 18 473 1512 14 8 1109 1745 2009 ## 24 0.435 0.565 edmonji01 2011 6858 1251 1949 437 25 393 1199 67 50 998 1729 2010 ## 25 0.427 0.573 abreubo01 2425 8480 1453 2470 574 59 288 1363 400 128 1476 1840 2014 ## 26 0.424 0.576 francju01 2527 8677 1285 2586 407 54 173 1194 281 107 917 1341 2007 ## 27 0.421 0.579 martied01 2055 7213 1219 2247 514 15 309 1261 49 30 1283 1202 2004 ## 28 0.417 0.583 finlest01 2583 9397 1443 2548 449 124 304 1167 320 118 844 1299 2007 ## 29 0.409 0.591 galaran01 2257 8096 1195 2333 444 32 399 1425 128 81 583 2003 2004 ## 30 0.402 0.598 aloumo01 1942 7037 1109 2134 421 39 332 1287 106 37 737 894 2008 ## 31 0.401 0.599 burksel01 2000 7232 1253 2107 402 63 352 1206 181 84 793 1340 2004 "]]
